{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bi8bolt/pytorch-ssd?scriptVersionId=127588400\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"7fd3a2a7","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:33.708889Z","iopub.status.busy":"2023-04-28T15:38:33.708456Z","iopub.status.idle":"2023-04-28T15:38:36.417863Z","shell.execute_reply":"2023-04-28T15:38:36.416851Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":2.724328,"end_time":"2023-04-28T15:38:36.420438","exception":false,"start_time":"2023-04-28T15:38:33.69611","status":"completed"},"tags":[]},"outputs":[],"source":["import argparse\n","import os\n","import logging\n","import sys\n","import itertools\n","import numpy as np\n","import pathlib\n","import xml.etree.ElementTree as ET\n","import cv2\n","import torch\n","import collections\n","import math\n","import time\n","import types\n","from torch.utils.data import DataLoader, ConcatDataset\n","from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n","from typing import List,Tuple\n","from torchvision import transforms\n","from numpy import random\n"]},{"cell_type":"code","execution_count":2,"id":"7ffd991d","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.440745Z","iopub.status.busy":"2023-04-28T15:38:36.440291Z","iopub.status.idle":"2023-04-28T15:38:36.446336Z","shell.execute_reply":"2023-04-28T15:38:36.444447Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.018031,"end_time":"2023-04-28T15:38:36.44826","exception":false,"start_time":"2023-04-28T15:38:36.430229","status":"completed"},"tags":[]},"outputs":[],"source":["image_size = 300\n","image_mean = np.array([127, 127, 127])  # RGB layout\n","image_std = 128.0\n","iou_threshold = 0.45\n","center_variance = 0.1\n","size_variance = 0.2\n","num_classes = 21"]},{"cell_type":"markdown","id":"de1d4994","metadata":{"papermill":{"duration":0.009022,"end_time":"2023-04-28T15:38:36.466357","exception":false,"start_time":"2023-04-28T15:38:36.457335","status":"completed"},"tags":[]},"source":["# **VOCDataset**"]},{"cell_type":"code","execution_count":3,"id":"764887bb","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.487715Z","iopub.status.busy":"2023-04-28T15:38:36.48704Z","iopub.status.idle":"2023-04-28T15:38:36.507245Z","shell.execute_reply":"2023-04-28T15:38:36.506388Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.033951,"end_time":"2023-04-28T15:38:36.509285","exception":false,"start_time":"2023-04-28T15:38:36.475334","status":"completed"},"tags":[]},"outputs":[],"source":["class VOCDataset:\n","\n","    def __init__(self, root, transform=None, target_transform=None, is_test=False, keep_difficult=False, label_file=None):\n","        \"\"\"Dataset for VOC data.\n","        Args:\n","            root: the root of the VOC2007 or VOC2012 dataset, the directory contains the following sub-directories:\n","                Annotations, ImageSets, JPEGImages, SegmentationClass, SegmentationObject.\n","        \"\"\"\n","        self.root = pathlib.Path(root)\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        if is_test:\n","            image_sets_file = self.root / \"ImageSets/Main/val.txt\"\n","        else:\n","            image_sets_file = self.root / \"ImageSets/Main/trainval.txt\"\n","        self.ids = VOCDataset._read_image_ids(image_sets_file)\n","        self.keep_difficult = keep_difficult\n","\n","        # if the labels file exists, read in the class names\n","        label_file_name = self.root / \"labels.txt\"\n","\n","        if os.path.isfile(label_file_name):\n","            class_string = \"\"\n","            with open(label_file_name, 'r') as infile:\n","                for line in infile:\n","                    class_string += line.rstrip()\n","\n","            # classes should be a comma separated list\n","            \n","            classes = class_string.split(',')\n","            # prepend BACKGROUND as first class\n","            classes.insert(0, 'BACKGROUND')\n","            classes  = [ elem.replace(\" \", \"\") for elem in classes]\n","            self.class_names = tuple(classes)\n","            logging.info(\"VOC Labels read from file: \" + str(self.class_names))\n","\n","        else:\n","            logging.info(\"No labels file, using default VOC classes.\")\n","            self.class_names = ('BACKGROUND',\n","            'aeroplane', 'bicycle', 'bird', 'boat',\n","            'bottle', 'bus', 'car', 'cat', 'chair',\n","            'cow', 'diningtable', 'dog', 'horse',\n","            'motorbike', 'person', 'pottedplant',\n","            'sheep', 'sofa', 'train', 'tvmonitor')\n","\n","\n","        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n","\n","    def __getitem__(self, index):\n","        image_id = self.ids[index]\n","        boxes, labels, is_difficult = self._get_annotation(image_id)\n","        if not self.keep_difficult:\n","            boxes = boxes[is_difficult == 0]\n","            labels = labels[is_difficult == 0]\n","        image = self._read_image(image_id)\n","        if self.transform:\n","            image, boxes, labels = self.transform(image, boxes, labels)\n","        if self.target_transform:\n","            boxes, labels = self.target_transform(boxes, labels)\n","        return image, boxes, labels\n","\n","    def get_image(self, index):\n","        image_id = self.ids[index]\n","        image = self._read_image(image_id)\n","        if self.transform:\n","            image, _ = self.transform(image)\n","        return image\n","\n","    def get_annotation(self, index):\n","        image_id = self.ids[index]\n","        return image_id, self._get_annotation(image_id)\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    @staticmethod\n","    def _read_image_ids(image_sets_file):\n","        ids = []\n","        with open(image_sets_file) as f:\n","            for line in f:\n","                ids.append(line.rstrip())\n","        return ids\n","\n","    def _get_annotation(self, image_id):\n","        annotation_file = self.root / f\"Annotations/{image_id}.xml\"\n","        objects = ET.parse(annotation_file).findall(\"object\")\n","        boxes = []\n","        labels = []\n","        is_difficult = []\n","        for object in objects:\n","            class_name = object.find('name').text.lower().strip()\n","            # we're only concerned with clases in our list\n","            if class_name in self.class_dict:\n","                bbox = object.find('bndbox')\n","\n","                # VOC dataset format follows Matlab, in which indexes start from 0\n","                x1 = float(bbox.find('xmin').text) - 1\n","                y1 = float(bbox.find('ymin').text) - 1\n","                x2 = float(bbox.find('xmax').text) - 1\n","                y2 = float(bbox.find('ymax').text) - 1\n","                boxes.append([x1, y1, x2, y2])\n","\n","                labels.append(self.class_dict[class_name])\n","                is_difficult_str = object.find('difficult').text\n","                is_difficult.append(int(is_difficult_str) if is_difficult_str else 0)\n","        \n","        # for i in boxes:\n","        #     i = np.array(i, dtype=np.float32)\n","        # for j in labels:\n","        #     j = np.array(j, dtype=np.int64)\n","\n","        return (np.array(boxes, dtype=np.float32),\n","                np.array(labels, dtype=np.int64),\n","                np.array(is_difficult, dtype=np.uint8))\n","\n","    def _read_image(self, image_id):\n","        image_file = self.root / f\"JPEGImages/{image_id}.jpg\"\n","        image = cv2.imread(str(image_file))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return image\n","\n"]},{"cell_type":"markdown","id":"c275d1e5","metadata":{"papermill":{"duration":0.00906,"end_time":"2023-04-28T15:38:36.527327","exception":false,"start_time":"2023-04-28T15:38:36.518267","status":"completed"},"tags":[]},"source":["# **transforms**"]},{"cell_type":"code","execution_count":4,"id":"b8c16c16","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.54704Z","iopub.status.busy":"2023-04-28T15:38:36.546773Z","iopub.status.idle":"2023-04-28T15:38:36.598043Z","shell.execute_reply":"2023-04-28T15:38:36.597186Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.063858,"end_time":"2023-04-28T15:38:36.60016","exception":false,"start_time":"2023-04-28T15:38:36.536302","status":"completed"},"tags":[]},"outputs":[],"source":["def intersect(box_a, box_b):\n","    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n","    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n","    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n","    return inter[:, 0] * inter[:, 1]\n","\n","\n","def jaccard_numpy(box_a, box_b):\n","    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n","    is simply the intersection over union of two boxes.\n","    E.g.:\n","        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n","    Args:\n","        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n","        box_b: Single bounding box, Shape: [4]\n","    Return:\n","        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n","    \"\"\"\n","    inter = intersect(box_a, box_b)\n","    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n","              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n","    area_b = ((box_b[2]-box_b[0]) *\n","              (box_b[3]-box_b[1]))  # [A,B]\n","    union = area_a + area_b - inter\n","    return inter / union  # [A,B]\n","\n","\n","class Compose(object):\n","    \"\"\"Composes several augmentations together.\n","    Args:\n","        transforms (List[Transform]): list of transforms to compose.\n","    Example:\n","        >>> augmentations.Compose([\n","        >>>     transforms.CenterCrop(10),\n","        >>>     transforms.ToTensor(),\n","        >>> ])\n","    \"\"\"\n","\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, boxes=None, labels=None):\n","        for t in self.transforms:\n","            img, boxes, labels = t(img, boxes, labels)\n","        return img, boxes, labels\n","\n","\n","class Lambda(object):\n","    \"\"\"Applies a lambda as a transform.\"\"\"\n","\n","    def __init__(self, lambd):\n","        assert isinstance(lambd, types.LambdaType)\n","        self.lambd = lambd\n","\n","    def __call__(self, img, boxes=None, labels=None):\n","        return self.lambd(img, boxes, labels)\n","\n","\n","class ConvertFromInts(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        return image.astype(np.float32), boxes, labels\n","\n","\n","class SubtractMeans(object):\n","    def __init__(self, mean):\n","        self.mean = np.array(mean, dtype=np.float32)\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        image = image.astype(np.float32)\n","        image -= self.mean\n","        return image.astype(np.float32), boxes, labels\n","\n","\n","class ToAbsoluteCoords(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, channels = image.shape\n","        boxes[:, 0] *= width\n","        boxes[:, 2] *= width\n","        boxes[:, 1] *= height\n","        boxes[:, 3] *= height\n","\n","        return image, boxes, labels\n","\n","\n","class ToPercentCoords(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, channels = image.shape\n","        boxes[:, 0] /= width\n","        boxes[:, 2] /= width\n","        boxes[:, 1] /= height\n","        boxes[:, 3] /= height\n","\n","        return image, boxes, labels\n","\n","\n","class Resize(object):\n","    def __init__(self, size=image_size):\n","        self.size = size\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        image = cv2.resize(image, (self.size,\n","                                 self.size))\n","        return image, boxes, labels\n","\n","\n","class RandomSaturation(object):\n","    def __init__(self, lower=0.5, upper=1.5):\n","        self.lower = lower\n","        self.upper = upper\n","        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n","        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n","\n","        return image, boxes, labels\n","\n","\n","class RandomHue(object):\n","    def __init__(self, delta=18.0):\n","        assert delta >= 0.0 and delta <= 360.0\n","        self.delta = delta\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n","            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n","            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n","        return image, boxes, labels\n","\n","\n","class RandomLightingNoise(object):\n","    def __init__(self):\n","        self.perms = ((0, 1, 2), (0, 2, 1),\n","                      (1, 0, 2), (1, 2, 0),\n","                      (2, 0, 1), (2, 1, 0))\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            swap = self.perms[random.randint(len(self.perms))]\n","            shuffle = SwapChannels(swap)  # shuffle channels\n","            image = shuffle(image)\n","        return image, boxes, labels\n","\n","\n","class ConvertColor(object):\n","    def __init__(self, current, transform):\n","        self.transform = transform\n","        self.current = current\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if self.current == 'BGR' and self.transform == 'HSV':\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","        elif self.current == 'RGB' and self.transform == 'HSV':\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n","        elif self.current == 'BGR' and self.transform == 'RGB':\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        elif self.current == 'HSV' and self.transform == 'BGR':\n","            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n","        elif self.current == 'HSV' and self.transform == \"RGB\":\n","            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n","        else:\n","            raise NotImplementedError\n","        return image, boxes, labels\n","\n","\n","class RandomContrast(object):\n","    def __init__(self, lower=0.5, upper=1.5):\n","        self.lower = lower\n","        self.upper = upper\n","        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n","        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n","\n","    # expects float image\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            alpha = random.uniform(self.lower, self.upper)\n","            image *= alpha\n","        return image, boxes, labels\n","\n","\n","class RandomBrightness(object):\n","    def __init__(self, delta=32):\n","        assert delta >= 0.0\n","        assert delta <= 255.0\n","        self.delta = delta\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            delta = random.uniform(-self.delta, self.delta)\n","            image += delta\n","        return image, boxes, labels\n","\n","\n","class ToCV2Image(object):\n","    def __call__(self, tensor, boxes=None, labels=None):\n","        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n","\n","\n","class ToTensor(object):\n","    def __call__(self, cvimage, boxes=None, labels=None):\n","        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n","\n","\n","class RandomSampleCrop(object):\n","    \"\"\"Crop\n","    Arguments:\n","        img (Image): the image being input during training\n","        boxes (Tensor): the original bounding boxes in pt form\n","        labels (Tensor): the class labels for each bbox\n","        mode (float tuple): the min and max jaccard overlaps\n","    Return:\n","        (img, boxes, classes)\n","            img (Image): the cropped image\n","            boxes (Tensor): the adjusted bounding boxes in pt form\n","            labels (Tensor): the class labels for each bbox\n","    \"\"\"\n","    def __init__(self):\n","        self.sample_options = (\n","            # using entire original input image\n","            None,\n","            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n","            (0.1, None),\n","            (0.3, None),\n","            (0.7, None),\n","            (0.9, None),\n","            # randomly sample a patch\n","            (None, None),\n","        )\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, _ = image.shape\n","        while True:\n","            # randomly choose a mode\n","            np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n","            mode = random.choice(self.sample_options)\n","            if mode is None:\n","                return image, boxes, labels\n","\n","            min_iou, max_iou = mode\n","            if min_iou is None:\n","                min_iou = float('-inf')\n","            if max_iou is None:\n","                max_iou = float('inf')\n","\n","            # max trails (50)\n","            for _ in range(50):\n","                current_image = image\n","\n","                w = random.uniform(0.3 * width, width)\n","                h = random.uniform(0.3 * height, height)\n","\n","                # aspect ratio constraint b/t .5 & 2\n","                if h / w < 0.5 or h / w > 2:\n","                    continue\n","\n","                left = random.uniform(width - w)\n","                top = random.uniform(height - h)\n","\n","                # convert to integer rect x1,y1,x2,y2\n","                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n","\n","                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n","                overlap = jaccard_numpy(boxes, rect)\n","\n","                # is min and max overlap constraint satisfied? if not try again\n","                if overlap.min() < min_iou and max_iou < overlap.max():\n","                    continue\n","\n","                # cut the crop from the image\n","                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n","                                              :]\n","\n","                # keep overlap with gt box IF center in sampled patch\n","                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n","\n","                # mask in all gt boxes that above and to the left of centers\n","                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n","\n","                # mask in all gt boxes that under and to the right of centers\n","                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n","\n","                # mask in that both m1 and m2 are true\n","                mask = m1 * m2\n","\n","                # have any valid boxes? try again if not\n","                if not mask.any():\n","                    continue\n","\n","                # take only matching gt boxes\n","                current_boxes = boxes[mask, :].copy()\n","\n","                # take only matching gt labels\n","                current_labels = labels[mask]\n","\n","                # should we use the box left and top corner or the crop's\n","                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n","                                                  rect[:2])\n","                # adjust to crop (by substracting crop's left,top)\n","                current_boxes[:, :2] -= rect[:2]\n","\n","                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n","                                                  rect[2:])\n","                # adjust to crop (by substracting crop's left,top)\n","                current_boxes[:, 2:] -= rect[:2]\n","\n","                return current_image, current_boxes, current_labels\n","\n","\n","class Expand(object):\n","    def __init__(self, mean):\n","        self.mean = mean\n","\n","    def __call__(self, image, boxes, labels):\n","        if random.randint(2):\n","            return image, boxes, labels\n","\n","        height, width, depth = image.shape\n","        ratio = random.uniform(1, 4)\n","        left = random.uniform(0, width*ratio - width)\n","        top = random.uniform(0, height*ratio - height)\n","\n","        expand_image = np.zeros(\n","            (int(height*ratio), int(width*ratio), depth),\n","            dtype=image.dtype)\n","        expand_image[:, :, :] = self.mean\n","        expand_image[int(top):int(top + height),\n","                     int(left):int(left + width)] = image\n","        image = expand_image\n","\n","        boxes = boxes.copy()\n","        boxes[:, :2] += (int(left), int(top))\n","        boxes[:, 2:] += (int(left), int(top))\n","\n","        return image, boxes, labels\n","\n","\n","class RandomMirror(object):\n","    def __call__(self, image, boxes, classes):\n","        _, width, _ = image.shape\n","        if random.randint(2):\n","            image = image[:, ::-1]\n","            boxes = boxes.copy()\n","            boxes[:, 0::2] = width - boxes[:, 2::-2]\n","        return image, boxes, classes\n","\n","\n","class SwapChannels(object):\n","    \"\"\"Transforms a tensorized image by swapping the channels in the order\n","     specified in the swap tuple.\n","    Args:\n","        swaps (int triple): final order of channels\n","            eg: (2, 1, 0)\n","    \"\"\"\n","\n","    def __init__(self, swaps):\n","        self.swaps = swaps\n","\n","    def __call__(self, image):\n","        \"\"\"\n","        Args:\n","            image (Tensor): image tensor to be transformed\n","        Return:\n","            a tensor with channels swapped according to swap\n","        \"\"\"\n","        # if torch.is_tensor(image):\n","        #     image = image.data.cpu().numpy()\n","        # else:\n","        #     image = np.array(image)\n","        image = image[:, :, self.swaps]\n","        return image\n","\n","\n","class PhotometricDistort(object):\n","    def __init__(self):\n","        self.pd = [\n","            RandomContrast(),  # RGB\n","            ConvertColor(current=\"RGB\", transform='HSV'),  # HSV\n","            RandomSaturation(),  # HSV\n","            RandomHue(),  # HSV\n","            ConvertColor(current='HSV', transform='RGB'),  # RGB\n","            RandomContrast()  # RGB\n","        ]\n","        self.rand_brightness = RandomBrightness()\n","        self.rand_light_noise = RandomLightingNoise()\n","\n","    def __call__(self, image, boxes, labels):\n","        im = image.copy()\n","        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n","        if random.randint(2):\n","            distort = Compose(self.pd[:-1])\n","        else:\n","            distort = Compose(self.pd[1:])\n","        im, boxes, labels = distort(im, boxes, labels)\n","        return self.rand_light_noise(im, boxes, labels)\n"]},{"cell_type":"code","execution_count":5,"id":"1268a019","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.620073Z","iopub.status.busy":"2023-04-28T15:38:36.619238Z","iopub.status.idle":"2023-04-28T15:38:36.626314Z","shell.execute_reply":"2023-04-28T15:38:36.625464Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.018969,"end_time":"2023-04-28T15:38:36.628289","exception":false,"start_time":"2023-04-28T15:38:36.60932","status":"completed"},"tags":[]},"outputs":[],"source":["SSDBoxSizes = collections.namedtuple('SSDBoxSizes', ['min', 'max'])\n","SSDSpec = collections.namedtuple('SSDSpec', ['feature_map_size', 'shrinkage', 'box_sizes', 'aspect_ratios'])\n","specs = [\n","    SSDSpec(19, 16, SSDBoxSizes(60, 105), [2, 3]),\n","    SSDSpec(10, 32, SSDBoxSizes(105, 150), [2, 3]),\n","    SSDSpec(5, 64, SSDBoxSizes(150, 195), [2, 3]),\n","    SSDSpec(3, 100, SSDBoxSizes(195, 240), [2, 3]),\n","    SSDSpec(2, 150, SSDBoxSizes(240, 285), [2, 3]),\n","    SSDSpec(1, 300, SSDBoxSizes(285, 330), [2, 3])\n","]\n","\n"]},{"cell_type":"markdown","id":"b4443ad4","metadata":{"papermill":{"duration":0.009061,"end_time":"2023-04-28T15:38:36.646262","exception":false,"start_time":"2023-04-28T15:38:36.637201","status":"completed"},"tags":[]},"source":["# **box_utils**"]},{"cell_type":"code","execution_count":6,"id":"a91e0756","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.665988Z","iopub.status.busy":"2023-04-28T15:38:36.665728Z","iopub.status.idle":"2023-04-28T15:38:36.696666Z","shell.execute_reply":"2023-04-28T15:38:36.695618Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.043459,"end_time":"2023-04-28T15:38:36.698653","exception":false,"start_time":"2023-04-28T15:38:36.655194","status":"completed"},"tags":[]},"outputs":[],"source":["def generate_ssd_priors(specs: List[SSDSpec], image_size, clamp=True) -> torch.Tensor:\n","    \"\"\"Generate SSD Prior Boxes.\n","\n","    It returns the center, height and width of the priors. The values are relative to the image size\n","    Args:\n","        specs: SSDSpecs about the shapes of sizes of prior boxes. i.e.\n","            specs = [\n","                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n","                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n","                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n","                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n","                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n","                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n","            ]\n","        image_size: image size.\n","        clamp: if true, clamp the values to make fall between [0.0, 1.0]\n","    Returns:\n","        priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n","            are relative to the image size.\n","    \"\"\"\n","    priors = []\n","    for spec in specs:\n","        scale = image_size / spec.shrinkage\n","        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n","            x_center = (i + 0.5) / scale\n","            y_center = (j + 0.5) / scale\n","\n","            # small sized square box\n","            size = spec.box_sizes.min\n","            h = w = size / image_size\n","            priors.append([\n","                x_center,\n","                y_center,\n","                w,\n","                h\n","            ])\n","\n","            # big sized square box\n","            size = math.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n","            h = w = size / image_size\n","            priors.append([\n","                x_center,\n","                y_center,\n","                w,\n","                h\n","            ])\n","\n","            # change h/w ratio of the small sized box\n","            size = spec.box_sizes.min\n","            h = w = size / image_size\n","            for ratio in spec.aspect_ratios:\n","                ratio = math.sqrt(ratio)\n","                priors.append([\n","                    x_center,\n","                    y_center,\n","                    w * ratio,\n","                    h / ratio\n","                ])\n","                priors.append([\n","                    x_center,\n","                    y_center,\n","                    w / ratio,\n","                    h * ratio\n","                ])\n","\n","    priors = torch.tensor(priors)\n","    if clamp:\n","        torch.clamp(priors, 0.0, 1.0, out=priors)\n","    return priors\n","def convert_locations_to_boxes(locations, priors, center_variance,\n","                               size_variance):\n","    \"\"\"Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n","\n","    The conversion:\n","        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n","        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n","    We do it in the inverse direction here.\n","    Args:\n","        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n","        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n","        center_variance: a float used to change the scale of center.\n","        size_variance: a float used to change of scale of size.\n","    Returns:\n","        boxes:  priors: [[center_x, center_y, h, w]]. All the values\n","            are relative to the image size.\n","    \"\"\"\n","    # priors can have one dimension less.\n","    if priors.dim() + 1 == locations.dim():\n","        priors = priors.unsqueeze(0)\n","    return torch.cat([\n","        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n","        torch.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n","    ], dim=locations.dim() - 1)\n","\n","\n","def convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n","    # priors can have one dimension less\n","    if center_form_priors.dim() + 1 == center_form_boxes.dim():\n","        center_form_priors = center_form_priors.unsqueeze(0)\n","    return torch.cat([\n","        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n","        torch.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n","    ], dim=center_form_boxes.dim() - 1)\n","\n","\n","def area_of(left_top, right_bottom) -> torch.Tensor:\n","    \"\"\"Compute the areas of rectangles given two corners.\n","\n","    Args:\n","        left_top (N, 2): left top corner.\n","        right_bottom (N, 2): right bottom corner.\n","\n","    Returns:\n","        area (N): return the area.\n","    \"\"\"\n","    hw = torch.clamp(right_bottom - left_top, min=0.0)\n","    return hw[..., 0] * hw[..., 1]\n","\n","\n","def iou_of(boxes0, boxes1, eps=1e-5):\n","    \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n","\n","    Args:\n","        boxes0 (N, 4): ground truth boxes.\n","        boxes1 (N or 1, 4): predicted boxes.\n","        eps: a small number to avoid 0 as denominator.\n","    Returns:\n","        iou (N): IoU values.\n","    \"\"\"\n","    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n","    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n","\n","    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n","    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n","    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n","    return overlap_area / (area0 + area1 - overlap_area + eps)\n","\n","\n","def assign_priors(gt_boxes, gt_labels, corner_form_priors,\n","                  iou_threshold):\n","    \"\"\"Assign ground truth boxes and targets to priors.\n","\n","    Args:\n","        gt_boxes (num_targets, 4): ground truth boxes.\n","        gt_labels (num_targets): labels of targets.\n","        priors (num_priors, 4): corner form priors\n","    Returns:\n","        boxes (num_priors, 4): real values for priors.\n","        labels (num_priros): labels for priors.\n","    \"\"\"\n","    # size: num_priors x num_targets\n","    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_priors.unsqueeze(1))\n","    # size: num_priors\n","    best_target_per_prior, best_target_per_prior_index = ious.max(1)\n","    # size: num_targets\n","    best_prior_per_target, best_prior_per_target_index = ious.max(0)\n","\n","    for target_index, prior_index in enumerate(best_prior_per_target_index):\n","        best_target_per_prior_index[prior_index] = target_index\n","    # 2.0 is used to make sure every target has a prior assigned\n","    best_target_per_prior.index_fill_(0, best_prior_per_target_index, 2)\n","    # size: num_priors\n","    labels = gt_labels[best_target_per_prior_index]\n","    labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n","    boxes = gt_boxes[best_target_per_prior_index]\n","    return boxes, labels\n","\n","\n","def hard_negative_mining(loss, labels, neg_pos_ratio):\n","    \"\"\"\n","    It used to suppress the presence of a large number of negative prediction.\n","    It works on image level not batch level.\n","    For any example/image, it keeps all the positive predictions and\n","     cut the number of negative predictions to make sure the ratio\n","     between the negative examples and positive examples is no more\n","     the given ratio for an image.\n","\n","    Args:\n","        loss (N, num_priors): the loss for each example.\n","        labels (N, num_priors): the labels.\n","        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n","    \"\"\"\n","    pos_mask = labels > 0\n","    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n","    num_neg = num_pos * neg_pos_ratio\n","\n","    loss[pos_mask] = -math.inf\n","    _, indexes = loss.sort(dim=1, descending=True)\n","    _, orders = indexes.sort(dim=1)\n","    neg_mask = orders < num_neg\n","    return pos_mask | neg_mask\n","\n","\n","def center_form_to_corner_form(locations):\n","    return torch.cat([locations[..., :2] - locations[..., 2:]/2,\n","                     locations[..., :2] + locations[..., 2:]/2], locations.dim() - 1) \n","\n","\n","def corner_form_to_center_form(boxes):\n","    return torch.cat([\n","        (boxes[..., :2] + boxes[..., 2:]) / 2,\n","         boxes[..., 2:] - boxes[..., :2]\n","    ], boxes.dim() - 1)\n","\n","\n","def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n","    \"\"\"\n","\n","    Args:\n","        box_scores (N, 5): boxes in corner-form and probabilities.\n","        iou_threshold: intersection over union threshold.\n","        top_k: keep top_k results. If k <= 0, keep all the results.\n","        candidate_size: only consider the candidates with the highest scores.\n","    Returns:\n","         picked: a list of indexes of the kept boxes\n","    \"\"\"\n","    scores = box_scores[:, -1]\n","    boxes = box_scores[:, :-1]\n","    picked = []\n","    _, indexes = scores.sort(descending=True)\n","    indexes = indexes[:candidate_size]\n","    while len(indexes) > 0:\n","        current = indexes[0]\n","        picked.append(current.item())\n","        if 0 < top_k == len(picked) or len(indexes) == 1:\n","            break\n","        current_box = boxes[current, :]\n","        indexes = indexes[1:]\n","        rest_boxes = boxes[indexes, :]\n","        iou = iou_of(\n","            rest_boxes,\n","            current_box.unsqueeze(0),\n","        )\n","        indexes = indexes[iou <= iou_threshold]\n","\n","    return box_scores[picked, :]\n","\n","\n","def nms(box_scores, nms_method=None, score_threshold=None, iou_threshold=None,\n","        sigma=0.5, top_k=-1, candidate_size=200):\n","    if nms_method == \"soft\":\n","        return soft_nms(box_scores, score_threshold, sigma, top_k)\n","    else:\n","        return hard_nms(box_scores, iou_threshold, top_k, candidate_size=candidate_size)\n","\n","\n","def soft_nms(box_scores, score_threshold, sigma=0.5, top_k=-1):\n","    \"\"\"Soft NMS implementation.\n","\n","    References:\n","        https://arxiv.org/abs/1704.04503\n","        https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/cython_nms.pyx\n","\n","    Args:\n","        box_scores (N, 5): boxes in corner-form and probabilities.\n","        score_threshold: boxes with scores less than value are not considered.\n","        sigma: the parameter in score re-computation.\n","            scores[i] = scores[i] * exp(-(iou_i)^2 / simga)\n","        top_k: keep top_k results. If k <= 0, keep all the results.\n","    Returns:\n","         picked_box_scores (K, 5): results of NMS.\n","    \"\"\"\n","    picked_box_scores = []\n","    while box_scores.size(0) > 0:\n","        max_score_index = torch.argmax(box_scores[:, 4])\n","        cur_box_prob = torch.tensor(box_scores[max_score_index, :])\n","        picked_box_scores.append(cur_box_prob)\n","        if len(picked_box_scores) == top_k > 0 or box_scores.size(0) == 1:\n","            break\n","        cur_box = cur_box_prob[:-1]\n","        box_scores[max_score_index, :] = box_scores[-1, :]\n","        box_scores = box_scores[:-1, :]\n","        ious = iou_of(cur_box.unsqueeze(0), box_scores[:, :-1])\n","        box_scores[:, -1] = box_scores[:, -1] * torch.exp(-(ious * ious) / sigma)\n","        box_scores = box_scores[box_scores[:, -1] > score_threshold, :]\n","    if len(picked_box_scores) > 0:\n","        return torch.stack(picked_box_scores)\n","    else:\n","        return torch.tensor([])\n"]},{"cell_type":"code","execution_count":7,"id":"b77cc5e5","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.71861Z","iopub.status.busy":"2023-04-28T15:38:36.718288Z","iopub.status.idle":"2023-04-28T15:38:36.750474Z","shell.execute_reply":"2023-04-28T15:38:36.749587Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.044486,"end_time":"2023-04-28T15:38:36.752591","exception":false,"start_time":"2023-04-28T15:38:36.708105","status":"completed"},"tags":[]},"outputs":[],"source":["priors = generate_ssd_priors(specs, image_size)"]},{"cell_type":"markdown","id":"88824774","metadata":{"papermill":{"duration":0.009059,"end_time":"2023-04-28T15:38:36.770649","exception":false,"start_time":"2023-04-28T15:38:36.76159","status":"completed"},"tags":[]},"source":["# **Predictor**"]},{"cell_type":"code","execution_count":8,"id":"e4c4a2e5","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.790086Z","iopub.status.busy":"2023-04-28T15:38:36.789809Z","iopub.status.idle":"2023-04-28T15:38:36.803379Z","shell.execute_reply":"2023-04-28T15:38:36.802357Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.026169,"end_time":"2023-04-28T15:38:36.805713","exception":false,"start_time":"2023-04-28T15:38:36.779544","status":"completed"},"tags":[]},"outputs":[],"source":["class Predictor:\n","    def __init__(self, net, size, mean=0.0, std=1.0, nms_method=None,\n","                 iou_threshold=0.45, filter_threshold=0.01, candidate_size=200, sigma=0.5, device=None):\n","        self.net = net\n","        self.transform = PredictionTransform(size, mean, std)\n","        self.iou_threshold = iou_threshold\n","        self.filter_threshold = filter_threshold\n","        self.candidate_size = candidate_size\n","        self.nms_method = nms_method\n","\n","        self.sigma = sigma\n","        if device:\n","            self.device = device\n","        else:\n","            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.net.to(self.device)\n","        self.net.eval()\n","\n","        self.timer = Timer()\n","\n","    def predict(self, image, top_k=-1, prob_threshold=None):\n","        cpu_device = torch.device(\"cpu\")\n","        height, width, _ = image.shape\n","        image = self.transform(image)\n","        images = image.unsqueeze(0)\n","        images = images.to(self.device)\n","        with torch.no_grad():\n","            self.timer.start()\n","            scores, boxes = self.net.forward(images)\n","            print(\"Inference time: \", self.timer.end())\n","        boxes = boxes[0]\n","        scores = scores[0]\n","        if not prob_threshold:\n","            prob_threshold = self.filter_threshold\n","        # this version of nms is slower on GPU, so we move data to CPU.\n","        boxes = boxes.to(cpu_device)\n","        scores = scores.to(cpu_device)\n","        picked_box_probs = []\n","        picked_labels = []\n","        for class_index in range(1, scores.size(1)):\n","            probs = scores[:, class_index]\n","            mask = probs > prob_threshold\n","            probs = probs[mask]\n","            if probs.size(0) == 0:\n","                continue\n","            subset_boxes = boxes[mask, :]\n","            box_probs = torch.cat([subset_boxes, probs.reshape(-1, 1)], dim=1)\n","            box_probs = nms(box_probs, self.nms_method,\n","                                      score_threshold=prob_threshold,\n","                                      iou_threshold=self.iou_threshold,\n","                                      sigma=self.sigma,\n","                                      top_k=top_k,\n","                                      candidate_size=self.candidate_size)\n","            picked_box_probs.append(box_probs)\n","            picked_labels.extend([class_index] * box_probs.size(0))\n","        if not picked_box_probs:\n","            return torch.tensor([]), torch.tensor([]), torch.tensor([])\n","        picked_box_probs = torch.cat(picked_box_probs)\n","        picked_box_probs[:, 0] *= width\n","        picked_box_probs[:, 1] *= height\n","        picked_box_probs[:, 2] *= width\n","        picked_box_probs[:, 3] *= height\n","        return picked_box_probs[:, :4], torch.tensor(picked_labels), picked_box_probs[:, 4]"]},{"cell_type":"markdown","id":"0344754f","metadata":{"papermill":{"duration":0.00886,"end_time":"2023-04-28T15:38:36.823466","exception":false,"start_time":"2023-04-28T15:38:36.814606","status":"completed"},"tags":[]},"source":["# **target_transform**"]},{"cell_type":"code","execution_count":9,"id":"30ccf996","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.843111Z","iopub.status.busy":"2023-04-28T15:38:36.842595Z","iopub.status.idle":"2023-04-28T15:38:36.849748Z","shell.execute_reply":"2023-04-28T15:38:36.848723Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.019156,"end_time":"2023-04-28T15:38:36.851802","exception":false,"start_time":"2023-04-28T15:38:36.832646","status":"completed"},"tags":[]},"outputs":[],"source":["class MatchPrior(object):\n","    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n","        self.center_form_priors = center_form_priors\n","        self.corner_form_priors = center_form_to_corner_form(center_form_priors)\n","        self.center_variance = center_variance\n","        self.size_variance = size_variance\n","        self.iou_threshold = iou_threshold\n","\n","    def __call__(self, gt_boxes, gt_labels):\n","        if type(gt_boxes) is np.ndarray:\n","            gt_boxes = torch.from_numpy(gt_boxes)\n","        if type(gt_labels) is np.ndarray:\n","            gt_labels = torch.from_numpy(gt_labels)\n","        boxes, labels = assign_priors(gt_boxes, gt_labels,\n","                                                self.corner_form_priors, self.iou_threshold)\n","        boxes = corner_form_to_center_form(boxes)\n","        locations = convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n","        return locations, labels"]},{"cell_type":"markdown","id":"c3c6a0ed","metadata":{"papermill":{"duration":0.009832,"end_time":"2023-04-28T15:38:36.870512","exception":false,"start_time":"2023-04-28T15:38:36.86068","status":"completed"},"tags":[]},"source":["# **train_transform**"]},{"cell_type":"code","execution_count":10,"id":"89f85bc5","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.890069Z","iopub.status.busy":"2023-04-28T15:38:36.889577Z","iopub.status.idle":"2023-04-28T15:38:36.896232Z","shell.execute_reply":"2023-04-28T15:38:36.895272Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.01877,"end_time":"2023-04-28T15:38:36.89827","exception":false,"start_time":"2023-04-28T15:38:36.8795","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainAugmentation:\n","    def __init__(self, size, mean=0, std=1.0):\n","        \"\"\"\n","        Args:\n","            size: the size the of final image.\n","            mean: mean pixel value per channel.\n","        \"\"\"\n","        self.mean = mean\n","        self.size = size\n","        self.augment = Compose([\n","            \n","            ConvertFromInts(),\n","            PhotometricDistort(),\n","            Expand(self.mean),\n","            RandomSampleCrop(),\n","            Resize(self.size),\n","            RandomMirror(),\n","            ToPercentCoords(),\n","            SubtractMeans(self.mean),\n","            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n","            ToTensor(),\n","        ])\n","\n","    def __call__(self, img, boxes, labels):\n","        \"\"\"\n","\n","        Args:\n","            img: the output of cv.imread in RGB layout.\n","            boxes: boundding boxes in the form of (x1, y1, x2, y2).\n","            labels: labels of boxes.\n","        \"\"\"\n","        return self.augment(img, boxes, labels)\n"]},{"cell_type":"markdown","id":"48a7b68e","metadata":{"papermill":{"duration":0.008979,"end_time":"2023-04-28T15:38:36.916321","exception":false,"start_time":"2023-04-28T15:38:36.907342","status":"completed"},"tags":[]},"source":["# **test_transform**"]},{"cell_type":"code","execution_count":11,"id":"8d4d708a","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.936101Z","iopub.status.busy":"2023-04-28T15:38:36.93532Z","iopub.status.idle":"2023-04-28T15:38:36.941589Z","shell.execute_reply":"2023-04-28T15:38:36.940736Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.018147,"end_time":"2023-04-28T15:38:36.943663","exception":false,"start_time":"2023-04-28T15:38:36.925516","status":"completed"},"tags":[]},"outputs":[],"source":["class TestTransform:\n","    def __init__(self, size, mean=0.0, std=1.0):\n","        self.transform = Compose([\n","            ToPercentCoords(),\n","            Resize(size),\n","            SubtractMeans(mean),\n","            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n","            ToTensor(),\n","        ])\n","\n","    def __call__(self, image, boxes, labels):\n","        return self.transform(image, boxes, labels)\n"]},{"cell_type":"markdown","id":"9e06f74b","metadata":{"papermill":{"duration":0.009055,"end_time":"2023-04-28T15:38:36.961925","exception":false,"start_time":"2023-04-28T15:38:36.95287","status":"completed"},"tags":[]},"source":["# **PredictionTransform**"]},{"cell_type":"code","execution_count":12,"id":"b689420c","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:36.981829Z","iopub.status.busy":"2023-04-28T15:38:36.981065Z","iopub.status.idle":"2023-04-28T15:38:36.987414Z","shell.execute_reply":"2023-04-28T15:38:36.986582Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.018295,"end_time":"2023-04-28T15:38:36.98945","exception":false,"start_time":"2023-04-28T15:38:36.971155","status":"completed"},"tags":[]},"outputs":[],"source":["class PredictionTransform:\n","    def __init__(self, size, mean=0.0, std=1.0):\n","        self.transform = Compose([\n","            Resize(size),\n","            SubtractMeans(mean),\n","            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n","            ToTensor()\n","        ])\n","\n","    def __call__(self, image):\n","        image, _, _ = self.transform(image)\n","        return image"]},{"cell_type":"markdown","id":"40ebf664","metadata":{"papermill":{"duration":0.009102,"end_time":"2023-04-28T15:38:37.007425","exception":false,"start_time":"2023-04-28T15:38:36.998323","status":"completed"},"tags":[]},"source":["# **misc**"]},{"cell_type":"code","execution_count":13,"id":"5cdd1855","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.026872Z","iopub.status.busy":"2023-04-28T15:38:37.02661Z","iopub.status.idle":"2023-04-28T15:38:37.035162Z","shell.execute_reply":"2023-04-28T15:38:37.034239Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.020705,"end_time":"2023-04-28T15:38:37.037159","exception":false,"start_time":"2023-04-28T15:38:37.016454","status":"completed"},"tags":[]},"outputs":[],"source":["def str2bool(s):\n","    return s.lower() in ('true', '1')\n","\n","\n","class Timer:\n","    def __init__(self):\n","        self.clock = {}\n","\n","    def start(self, key=\"default\"):\n","        self.clock[key] = time.time()\n","\n","    def end(self, key=\"default\"):\n","        if key not in self.clock:\n","            raise Exception(f\"{key} is not in the clock.\")\n","        interval = time.time() - self.clock[key]\n","        del self.clock[key]\n","        return interval\n","        \n","\n","def save_checkpoint(epoch, net_state_dict, optimizer_state_dict, best_score, checkpoint_path, model_path):\n","    torch.save({\n","        'epoch': epoch,\n","        'model': net_state_dict,\n","        'optimizer': optimizer_state_dict,\n","        'best_score': best_score\n","    }, checkpoint_path)\n","    torch.save(net_state_dict, model_path)\n","        \n","        \n","def load_checkpoint(checkpoint_path):\n","    return torch.load(checkpoint_path)\n","\n","\n","def freeze_net_layers(net):\n","    for param in net.parameters():\n","        param.requires_grad = False\n","\n","\n","def store_labels(path, labels):\n","    with open(path, \"w\") as f:\n","        f.write(\"\\n\".join(labels))\n"]},{"cell_type":"code","execution_count":14,"id":"32875314","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.056635Z","iopub.status.busy":"2023-04-28T15:38:37.056355Z","iopub.status.idle":"2023-04-28T15:38:37.130766Z","shell.execute_reply":"2023-04-28T15:38:37.129735Z"},"papermill":{"duration":0.086915,"end_time":"2023-04-28T15:38:37.133172","exception":false,"start_time":"2023-04-28T15:38:37.046257","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Use Cuda.\n"]}],"source":["parser = argparse.ArgumentParser(\n","    description='Single Shot MultiBox Detector Training With Pytorch')\n","\n","parser.add_argument(\"--dataset_type\", default=\"voc\", type=str,\n","                    help='Specify dataset type. Currently support voc and open_images.')\n","\n","parser.add_argument('--datasets', nargs='+', help='Dataset directory path')\n","parser.add_argument('--validation_dataset', help='Dataset directory path')\n","parser.add_argument('--balance_data', action='store_true',\n","                    help=\"Balance training data by down-sampling more frequent labels.\")\n","\n","\n","parser.add_argument('--net', default=\"mb1-ssd\",\n","                    help=\"The network architecture, it can be mb1-ssd, mb1-lite-ssd, mb2-ssd-lite, mb3-large-ssd-lite, mb3-small-ssd-lite or vgg16-ssd.\")\n","parser.add_argument('--freeze_base_net', action='store_true',\n","                    help=\"Freeze base net layers.\")\n","parser.add_argument('--freeze_net', action='store_true',\n","                    help=\"Freeze all the layers except the prediction head.\")\n","\n","parser.add_argument('--mb2_width_mult', default=1.0, type=float,\n","                    help='Width Multiplifier for MobilenetV2')\n","\n","# Params for SGD\n","parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n","                    help='initial learning rate')\n","parser.add_argument('--momentum', default=0.9, type=float,\n","                    help='Momentum value for optim')\n","parser.add_argument('--weight_decay', default=5e-5, type=float,\n","                    help='Weight decay for SGD')\n","parser.add_argument('--gamma', default=0.1, type=float,\n","                    help='Gamma update for SGD')\n","parser.add_argument('--base_net_lr', default=0.001, type=float,\n","                    help='initial learning rate for base net.')\n","parser.add_argument('--extra_layers_lr', default=0.001, type=float,\n","                    help='initial learning rate for the layers not in base net and prediction heads.')\n","\n","\n","# Params for loading pretrained basenet or checkpoints.\n","parser.add_argument('--base_net',\n","                    help='Pretrained base model')\n","parser.add_argument('--pretrained_ssd', help='Pre-trained base model')\n","parser.add_argument('--resume', default=None, type=str,\n","                    help='Checkpoint state_dict file to resume training from')\n","\n","# Scheduler\n","parser.add_argument('--scheduler', default=\"multi-step\", type=str,\n","                    help=\"Scheduler for SGD. It can one of multi-step and cosine\")\n","\n","# Params for Multi-step Scheduler\n","parser.add_argument('--milestones', default=\"80,100\", type=str,\n","                    help=\"milestones for MultiStepLR\")\n","\n","# Params for Cosine Annealing\n","parser.add_argument('--t_max', default=100, type=float,\n","                    help='T_max value for Cosine Annealing Scheduler.')\n","\n","# Train params\n","parser.add_argument('--batch_size', default=128, type=int,\n","                    help='Batch size for training')\n","parser.add_argument('--num_epochs', default=200, type=int,\n","                    help='the number epochs')\n","parser.add_argument('--num_workers', default=4, type=int,\n","                    help='Number of workers used in dataloading')\n","parser.add_argument('--validation_epochs', default=10, type=int,\n","                    help='the number epochs')\n","parser.add_argument('--debug_steps', default=90, type=int,\n","                    help='Set the debug log output frequency.')\n","parser.add_argument('--use_cuda', default=True, type=str2bool,\n","                    help='Use CUDA to train model')\n","\n","parser.add_argument('--checkpoint_folder', default='/kaggle/working/VOC2012/',\n","                    help='Directory for saving checkpoint models')\n","\n","\n","logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n","                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","args = parser.parse_args(args=[])\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n","\n","if args.use_cuda and torch.cuda.is_available():\n","    torch.backends.cudnn.benchmark = True\n","    print(\"Use Cuda.\")\n"]},{"cell_type":"code","execution_count":15,"id":"b8630792","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.153249Z","iopub.status.busy":"2023-04-28T15:38:37.152427Z","iopub.status.idle":"2023-04-28T15:38:37.15805Z","shell.execute_reply":"2023-04-28T15:38:37.157193Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.017754,"end_time":"2023-04-28T15:38:37.160018","exception":false,"start_time":"2023-04-28T15:38:37.142264","status":"completed"},"tags":[]},"outputs":[],"source":["dataset_path = '/kaggle/input/pascal-voc-2012/VOC2012'"]},{"cell_type":"code","execution_count":16,"id":"f2ae4b05","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.179773Z","iopub.status.busy":"2023-04-28T15:38:37.179233Z","iopub.status.idle":"2023-04-28T15:38:37.218904Z","shell.execute_reply":"2023-04-28T15:38:37.218038Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.051578,"end_time":"2023-04-28T15:38:37.220864","exception":false,"start_time":"2023-04-28T15:38:37.169286","status":"completed"},"tags":[]},"outputs":[],"source":["train_transform = TrainAugmentation(image_size, image_mean, image_std)\n","target_transform = MatchPrior(priors, center_variance, size_variance, 0.5)\n","test_transform = TestTransform(image_size, image_mean, image_std)"]},{"cell_type":"markdown","id":"1f031af8","metadata":{"papermill":{"duration":0.008775,"end_time":"2023-04-28T15:38:37.23918","exception":false,"start_time":"2023-04-28T15:38:37.230405","status":"completed"},"tags":[]},"source":["# **train_loader**"]},{"cell_type":"code","execution_count":17,"id":"91513b35","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.258934Z","iopub.status.busy":"2023-04-28T15:38:37.258311Z","iopub.status.idle":"2023-04-28T15:38:37.291526Z","shell.execute_reply":"2023-04-28T15:38:37.290392Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.045624,"end_time":"2023-04-28T15:38:37.29404","exception":false,"start_time":"2023-04-28T15:38:37.248416","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size: 11540\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["train_dataset = VOCDataset(dataset_path, transform=train_transform,\n","                             target_transform=target_transform)\n","#train_dataset = ConcatDataset(dataset)\n","train_loader = DataLoader(train_dataset, args.batch_size,\n","                          num_workers=args.num_workers,\n","                          shuffle=True)\n","print(\"Train dataset size: {}\".format(len(train_dataset)))"]},{"cell_type":"code","execution_count":18,"id":"bbfb4c75","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.313803Z","iopub.status.busy":"2023-04-28T15:38:37.313284Z","iopub.status.idle":"2023-04-28T15:38:37.487504Z","shell.execute_reply":"2023-04-28T15:38:37.486235Z"},"papermill":{"duration":0.186551,"end_time":"2023-04-28T15:38:37.489811","exception":false,"start_time":"2023-04-28T15:38:37.30326","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(tensor([[[-0.1761, -0.1761, -0.1761,  ...,  0.2359,  0.3001,  0.3359],\n","         [-0.1874, -0.1840, -0.1781,  ...,  0.2596,  0.2760,  0.2851],\n","         [-0.1971, -0.1941, -0.1887,  ...,  0.2500,  0.2329,  0.2234],\n","         ...,\n","         [ 0.5425,  0.5323,  0.5138,  ...,  0.2367,  0.1900,  0.1641],\n","         [ 0.5172,  0.5123,  0.5035,  ...,  0.2942,  0.2600,  0.2409],\n","         [ 0.4835,  0.4886,  0.4978,  ...,  0.3745,  0.3631,  0.3567]],\n","\n","        [[-0.1941, -0.1915, -0.1867,  ...,  0.3503,  0.4193,  0.4577],\n","         [-0.2054, -0.1986, -0.1862,  ...,  0.3740,  0.3951,  0.4069],\n","         [-0.2151, -0.2081, -0.1956,  ...,  0.3625,  0.3502,  0.3434],\n","         ...,\n","         [ 0.6779,  0.6676,  0.6492,  ...,  0.3087,  0.2651,  0.2408],\n","         [ 0.6494,  0.6445,  0.6358,  ...,  0.3764,  0.3346,  0.3113],\n","         [ 0.6099,  0.6150,  0.6242,  ...,  0.4666,  0.4337,  0.4154]],\n","\n","        [[-0.1896, -0.1870, -0.1822,  ...,  0.0814,  0.1289,  0.1554],\n","         [-0.2009, -0.1966, -0.1890,  ...,  0.1076,  0.1056,  0.1046],\n","         [-0.2144, -0.2103, -0.2029,  ...,  0.1096,  0.0704,  0.0486],\n","         ...,\n","         [ 0.7862,  0.7759,  0.7575,  ...,  0.3602,  0.3299,  0.3130],\n","         [ 0.7577,  0.7528,  0.7441,  ...,  0.4284,  0.4026,  0.3882],\n","         [ 0.7182,  0.7233,  0.7325,  ...,  0.5226,  0.5088,  0.5011]]]), tensor([[11.9167, 17.8333,  4.8728,  6.7187],\n","        [ 9.0082, 13.4807,  3.4738,  5.3196],\n","        [ 8.4264, 25.2201,  3.1399,  8.4515],\n","        ...,\n","        [-3.4983, -1.1667, -1.1851, -1.3285],\n","        [-2.3500, -2.1271, -3.1744,  1.6745],\n","        [-4.2845, -1.1667, -0.1714, -1.3285]]), tensor([0, 0, 0,  ..., 0, 0, 0]))\n"]}],"source":["print(train_dataset[10])"]},{"cell_type":"markdown","id":"1791fbb5","metadata":{"papermill":{"duration":0.009339,"end_time":"2023-04-28T15:38:37.509034","exception":false,"start_time":"2023-04-28T15:38:37.499695","status":"completed"},"tags":[]},"source":["# **val_loader**"]},{"cell_type":"code","execution_count":19,"id":"3351a017","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.529663Z","iopub.status.busy":"2023-04-28T15:38:37.5287Z","iopub.status.idle":"2023-04-28T15:38:37.546192Z","shell.execute_reply":"2023-04-28T15:38:37.544872Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.030232,"end_time":"2023-04-28T15:38:37.548504","exception":false,"start_time":"2023-04-28T15:38:37.518272","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset size: 5823\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["val_dataset = VOCDataset(dataset_path, transform=test_transform,\n","                                 target_transform=target_transform, is_test=True)\n","val_loader = DataLoader(val_dataset, args.batch_size,\n","                        num_workers=args.num_workers,\n","                        shuffle=False)\n","print(\"Test dataset size: {}\".format(len(val_dataset)))"]},{"cell_type":"markdown","id":"0b8e7e0b","metadata":{"papermill":{"duration":0.009545,"end_time":"2023-04-28T15:38:37.56738","exception":false,"start_time":"2023-04-28T15:38:37.557835","status":"completed"},"tags":[]},"source":["# **import**"]},{"cell_type":"code","execution_count":20,"id":"bad9ba2c","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.587995Z","iopub.status.busy":"2023-04-28T15:38:37.587222Z","iopub.status.idle":"2023-04-28T15:38:37.592036Z","shell.execute_reply":"2023-04-28T15:38:37.59104Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.017212,"end_time":"2023-04-28T15:38:37.59407","exception":false,"start_time":"2023-04-28T15:38:37.576858","status":"completed"},"tags":[]},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import namedtuple\n","from torch.nn import Conv2d, Sequential, ModuleList, ReLU, BatchNorm2d"]},{"cell_type":"markdown","id":"cdeb3d93","metadata":{"papermill":{"duration":0.009443,"end_time":"2023-04-28T15:38:37.612838","exception":false,"start_time":"2023-04-28T15:38:37.603395","status":"completed"},"tags":[]},"source":["# **MobileNet**"]},{"cell_type":"code","execution_count":21,"id":"65be08d7","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.633484Z","iopub.status.busy":"2023-04-28T15:38:37.632637Z","iopub.status.idle":"2023-04-28T15:38:37.643853Z","shell.execute_reply":"2023-04-28T15:38:37.642974Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.023892,"end_time":"2023-04-28T15:38:37.646128","exception":false,"start_time":"2023-04-28T15:38:37.622236","status":"completed"},"tags":[]},"outputs":[],"source":["class MobileNetV1(nn.Module):\n","    def __init__(self, num_classes=21):\n","        super(MobileNetV1, self).__init__()\n","\n","        def conv_bn(inp, oup, stride):\n","            return nn.Sequential(\n","                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n","                nn.BatchNorm2d(oup),\n","                nn.ReLU(inplace=True)\n","            )\n","\n","        def conv_dw(inp, oup, stride):\n","            return nn.Sequential(\n","                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n","                nn.BatchNorm2d(inp),\n","                nn.ReLU(inplace=True),\n","\n","                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","                nn.ReLU(inplace=True),\n","            )\n","\n","        self.model = nn.Sequential(\n","            conv_bn(3, 32, 2),\n","            conv_dw(32, 64, 1),\n","            conv_dw(64, 128, 2),\n","            conv_dw(128, 128, 1),\n","            conv_dw(128, 256, 2),\n","            conv_dw(256, 256, 1),\n","            conv_dw(256, 512, 2),\n","            conv_dw(512, 512, 1),\n","            conv_dw(512, 512, 1),\n","            conv_dw(512, 512, 1),\n","            conv_dw(512, 512, 1),\n","            conv_dw(512, 512, 1),\n","            conv_dw(512, 1024, 2),\n","            conv_dw(1024, 1024, 1),\n","        )\n","        self.fc = nn.Linear(1024, num_classes)\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = F.avg_pool2d(x, 7)\n","        x = x.view(-1, 1024)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"markdown","id":"1d4d7eba","metadata":{"papermill":{"duration":0.009232,"end_time":"2023-04-28T15:38:37.664638","exception":false,"start_time":"2023-04-28T15:38:37.655406","status":"completed"},"tags":[]},"source":["# **SSD**"]},{"cell_type":"code","execution_count":22,"id":"48fbfdfd","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.685273Z","iopub.status.busy":"2023-04-28T15:38:37.684786Z","iopub.status.idle":"2023-04-28T15:38:37.711239Z","shell.execute_reply":"2023-04-28T15:38:37.710385Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.039231,"end_time":"2023-04-28T15:38:37.713316","exception":false,"start_time":"2023-04-28T15:38:37.674085","status":"completed"},"tags":[]},"outputs":[],"source":["\n","GraphPath = namedtuple(\"GraphPath\", ['s0', 'name', 's1'])  #\n","\n","class SSD(nn.Module):\n","    def __init__(self, num_classes: int, base_net: nn.ModuleList, source_layer_indexes: List[int],\n","                 extras: nn.ModuleList, classification_headers: nn.ModuleList,\n","                 regression_headers: nn.ModuleList, center_variance: nn.ModuleList, size_variance: nn.ModuleList, is_test=False, device=None):\n","        \"\"\"Compose a SSD model using the given components.\n","        \"\"\"\n","        super(SSD, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.base_net = base_net\n","        self.source_layer_indexes = source_layer_indexes\n","        self.extras = extras\n","        self.classification_headers = classification_headers\n","        self.regression_headers = regression_headers\n","        self.is_test = is_test\n","\n","        # register layers in source_layer_indexes by adding them to a module list\n","        self.source_layer_add_ons = nn.ModuleList([t[1] for t in source_layer_indexes\n","                                                   if isinstance(t, tuple) and not isinstance(t, GraphPath)])\n","        if device:\n","            self.device = device\n","        else:\n","            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        if is_test:\n","            self.priors = priors.to(self.device)\n","            self.center_variance = center_variance\n","            self.size_variance = size_variance\n","            \n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        confidences = []\n","        locations = []\n","        start_layer_index = 0\n","        header_index = 0\n","        for end_layer_index in self.source_layer_indexes:\n","            if isinstance(end_layer_index, GraphPath):\n","                path = end_layer_index\n","                end_layer_index = end_layer_index.s0\n","                added_layer = None\n","            elif isinstance(end_layer_index, tuple):\n","                added_layer = end_layer_index[1]\n","                end_layer_index = end_layer_index[0]\n","                path = None\n","            else:\n","                added_layer = None\n","                path = None\n","            for layer in self.base_net[start_layer_index: end_layer_index]:\n","                x = layer(x)\n","            if added_layer:\n","                y = added_layer(x)\n","            else:\n","                y = x\n","            if path:\n","                sub = getattr(self.base_net[end_layer_index], path.name)\n","                for layer in sub[:path.s1]:\n","                    x = layer(x)\n","                y = x\n","                for layer in sub[path.s1:]:\n","                    x = layer(x)\n","                end_layer_index += 1\n","            start_layer_index = end_layer_index\n","            confidence, location = self.compute_header(header_index, y)\n","            header_index += 1\n","            confidences.append(confidence)\n","            locations.append(location)\n","\n","        for layer in self.base_net[end_layer_index:]:\n","            x = layer(x)\n","\n","        for layer in self.extras:\n","            x = layer(x)\n","            confidence, location = self.compute_header(header_index, x)\n","            header_index += 1\n","            confidences.append(confidence)\n","            locations.append(location)\n","\n","        confidences = torch.cat(confidences, 1)\n","        locations = torch.cat(locations, 1)\n","        \n","        if self.is_test:\n","            confidences = F.softmax(confidences, dim=2)\n","            boxes = convert_locations_to_boxes(\n","                locations, self.priors, self.center_variance, self.size_variance\n","            )\n","            boxes = center_form_to_corner_form(boxes)\n","            return confidences, boxes\n","        else:\n","            return confidences, locations\n","\n","    def compute_header(self, i, x):\n","        confidence = self.classification_headers[i](x)\n","        confidence = confidence.permute(0, 2, 3, 1).contiguous()\n","        confidence = confidence.view(confidence.size(0), -1, self.num_classes)\n","\n","        location = self.regression_headers[i](x)\n","        location = location.permute(0, 2, 3, 1).contiguous()\n","        location = location.view(location.size(0), -1, 4)\n","\n","        return confidence, location\n","\n","    def init_from_base_net(self, model):\n","        self.base_net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage), strict=True)\n","        self.source_layer_add_ons.apply(_xavier_init_)\n","        self.extras.apply(_xavier_init_)\n","        self.classification_headers.apply(_xavier_init_)\n","        self.regression_headers.apply(_xavier_init_)\n","\n","    def init_from_pretrained_ssd(self, model):\n","        state_dict = torch.load(model, map_location=lambda storage, loc: storage)\n","        state_dict = {k: v for k, v in state_dict.items() if not (k.startswith(\"classification_headers\") or k.startswith(\"regression_headers\"))}\n","        model_dict = self.state_dict()\n","        model_dict.update(state_dict)\n","        self.load_state_dict(model_dict)\n","        self.classification_headers.apply(_xavier_init_)\n","        self.regression_headers.apply(_xavier_init_)\n","\n","    def init(self):\n","        self.base_net.apply(_xavier_init_)\n","        self.source_layer_add_ons.apply(_xavier_init_)\n","        self.extras.apply(_xavier_init_)\n","        self.classification_headers.apply(_xavier_init_)\n","        self.regression_headers.apply(_xavier_init_)\n","\n","    def load(self, model):\n","        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n","\n","    def save(self, model_path):\n","        torch.save(self.state_dict(), model_path)\n","\n","\n","class MatchPrior(object):\n","    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n","        self.center_form_priors = center_form_priors\n","        self.corner_form_priors = center_form_to_corner_form(center_form_priors)\n","        self.center_variance = center_variance\n","        self.size_variance = size_variance\n","        self.iou_threshold = iou_threshold\n","\n","    def __call__(self, gt_boxes, gt_labels):\n","        if type(gt_boxes) is np.ndarray:\n","            gt_boxes = torch.from_numpy(gt_boxes)\n","        if type(gt_labels) is np.ndarray:\n","            gt_labels = torch.from_numpy(gt_labels)\n","        boxes, labels = assign_priors(gt_boxes, gt_labels,\n","                                                self.corner_form_priors, self.iou_threshold)\n","        boxes = corner_form_to_center_form(boxes)\n","        locations = convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n","        return locations, labels\n","\n","\n","def _xavier_init_(m: nn.Module):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.xavier_uniform_(m.weight)\n"]},{"cell_type":"code","execution_count":23,"id":"0ad6c39c","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.733881Z","iopub.status.busy":"2023-04-28T15:38:37.733024Z","iopub.status.idle":"2023-04-28T15:38:37.746821Z","shell.execute_reply":"2023-04-28T15:38:37.745985Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.02619,"end_time":"2023-04-28T15:38:37.748858","exception":false,"start_time":"2023-04-28T15:38:37.722668","status":"completed"},"tags":[]},"outputs":[],"source":["def SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n","    \"\"\"Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n","    \"\"\"\n","    return Sequential(\n","        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n","               groups=in_channels, stride=stride, padding=padding),\n","        ReLU(),\n","        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n","    )\n","\n","\n","def create_mobilenetv1_ssd(num_classes, is_test=False):\n","    base_net = MobileNetV1().model  # disable dropout layer\n","\n","    source_layer_indexes = [\n","        12,\n","        14,\n","    ]\n","    extras = ModuleList([\n","        Sequential(\n","            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n","            ReLU(),\n","            Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n","            ReLU()\n","        ),\n","        Sequential(\n","            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n","            ReLU(),\n","            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n","            ReLU()\n","        ),\n","        Sequential(\n","            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n","            ReLU(),\n","            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n","            ReLU()\n","        ),\n","        Sequential(\n","            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n","            ReLU(),\n","            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n","            ReLU()\n","        )\n","    ])\n","\n","    regression_headers = ModuleList([\n","        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n","        Conv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1),\n","        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n","    ])\n","\n","    classification_headers = ModuleList([\n","        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n","        Conv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1),\n","        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n","        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n","    ])\n","\n","    return SSD(num_classes, base_net, source_layer_indexes,\n","               extras, classification_headers, regression_headers, center_variance, size_variance, is_test=is_test)"]},{"cell_type":"code","execution_count":24,"id":"376710f0","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.769094Z","iopub.status.busy":"2023-04-28T15:38:37.76844Z","iopub.status.idle":"2023-04-28T15:38:37.876536Z","shell.execute_reply":"2023-04-28T15:38:37.875503Z"},"papermill":{"duration":0.120858,"end_time":"2023-04-28T15:38:37.879001","exception":false,"start_time":"2023-04-28T15:38:37.758143","status":"completed"},"tags":[]},"outputs":[],"source":["create_net = create_mobilenetv1_ssd\n","net = create_net(21)\n","min_loss = -10000.0\n","last_epoch = -1"]},{"cell_type":"markdown","id":"dbb0da3a","metadata":{"papermill":{"duration":0.009184,"end_time":"2023-04-28T15:38:37.898334","exception":false,"start_time":"2023-04-28T15:38:37.88915","status":"completed"},"tags":[]},"source":["# **MultiboxLoss**"]},{"cell_type":"code","execution_count":25,"id":"da9eef9a","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.91926Z","iopub.status.busy":"2023-04-28T15:38:37.918355Z","iopub.status.idle":"2023-04-28T15:38:37.928373Z","shell.execute_reply":"2023-04-28T15:38:37.927497Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.022486,"end_time":"2023-04-28T15:38:37.930453","exception":false,"start_time":"2023-04-28T15:38:37.907967","status":"completed"},"tags":[]},"outputs":[],"source":["class MultiboxLoss(nn.Module):\n","    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n","                 center_variance, size_variance, device):\n","        \"\"\"Implement SSD Multibox Loss.\n","\n","        Basically, Multibox loss combines classification loss\n","         and Smooth L1 regression loss.\n","        \"\"\"\n","        super(MultiboxLoss, self).__init__()\n","        self.iou_threshold = iou_threshold\n","        self.neg_pos_ratio = neg_pos_ratio\n","        self.center_variance = center_variance\n","        self.size_variance = size_variance\n","        self.priors = priors\n","        self.priors.to(device)\n","\n","    def forward(self, confidence, predicted_locations, labels, gt_locations):\n","        \"\"\"Compute classification loss and smooth l1 loss.\n","\n","        Args:\n","            confidence (batch_size, num_priors, num_classes): class predictions.\n","            locations (batch_size, num_priors, 4): predicted locations.\n","            labels (batch_size, num_priors): real labels of all the priors.\n","            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.\n","        \"\"\"\n","        num_classes = confidence.size(2)\n","        with torch.no_grad():\n","            # derived from cross_entropy=sum(log(p))\n","            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]\n","            mask = hard_negative_mining(loss, labels, self.neg_pos_ratio)\n","\n","        confidence = confidence[mask, :]\n","        classification_loss = F.cross_entropy(confidence.reshape(-1, num_classes), labels[mask],  size_average=False)\n","        pos_mask = labels > 0\n","        predicted_locations = predicted_locations[pos_mask, :].reshape(-1, 4)\n","        gt_locations = gt_locations[pos_mask, :].reshape(-1, 4)\n","        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations,  size_average=False)\n","        num_pos = gt_locations.size(0)\n","        return smooth_l1_loss/num_pos, classification_loss/num_pos\n"]},{"cell_type":"code","execution_count":26,"id":"7419657b","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.950534Z","iopub.status.busy":"2023-04-28T15:38:37.950262Z","iopub.status.idle":"2023-04-28T15:38:37.96282Z","shell.execute_reply":"2023-04-28T15:38:37.961746Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.025403,"end_time":"2023-04-28T15:38:37.965266","exception":false,"start_time":"2023-04-28T15:38:37.939863","status":"completed"},"tags":[]},"outputs":[],"source":["def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n","    net.train(True)\n","    running_loss = 0.0\n","    running_regression_loss = 0.0\n","    running_classification_loss = 0.0\n","    for i, data in enumerate(loader):\n","        images, boxes, labels = data\n","        images = images.to(device)\n","        boxes = boxes.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        confidence, locations = net(images)\n","        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)  # TODO CHANGE BOXES\n","        loss = regression_loss + classification_loss\n","        loss.requires_grad_(True)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        running_regression_loss += regression_loss.item()\n","        running_classification_loss += classification_loss.item()\n","        if i and i % debug_steps == 0:\n","            avg_loss = running_loss / debug_steps\n","            avg_reg_loss = running_regression_loss / debug_steps\n","            avg_clf_loss = running_classification_loss / debug_steps\n","            print(\n","                f\"Epoch: {epoch}, Step: {i}, \" +\n","                f\"Average Loss: {avg_loss:.4f}, \" +\n","                f\"Average Regression Loss {avg_reg_loss:.4f}, \" +\n","                f\"Average Classification Loss: {avg_clf_loss:.4f}\"\n","            )\n","            running_loss = 0.0\n","            running_regression_loss = 0.0\n","            running_classification_loss = 0.0\n","\n","\n","def test(loader, net, criterion, device):\n","    net.eval()\n","    running_loss = 0.0\n","    running_regression_loss = 0.0\n","    running_classification_loss = 0.0\n","    num = 0\n","    for _, data in enumerate(loader):\n","        images, boxes, labels = data\n","        images = images.to(device)\n","        boxes = boxes.to(device)\n","        labels = labels.to(device)\n","        num += 1\n","\n","        with torch.no_grad():\n","            confidence, locations = net(images)\n","            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n","            loss = regression_loss + classification_loss\n","\n","        running_loss += loss.item()\n","        running_regression_loss += regression_loss.item()\n","        running_classification_loss += classification_loss.item()\n","    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n","\n"]},{"cell_type":"code","execution_count":27,"id":"aac5d3dc","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:37.985703Z","iopub.status.busy":"2023-04-28T15:38:37.984864Z","iopub.status.idle":"2023-04-28T15:38:37.993985Z","shell.execute_reply":"2023-04-28T15:38:37.993124Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.02139,"end_time":"2023-04-28T15:38:37.996011","exception":false,"start_time":"2023-04-28T15:38:37.974621","status":"completed"},"tags":[]},"outputs":[],"source":["base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr\n","extra_layers_lr = args.extra_layers_lr if args.extra_layers_lr is not None else args.lr\n","if args.freeze_base_net:\n","    logging.info(\"Freeze base net.\")\n","    freeze_net_layers(net.base_net)\n","    params = itertools.chain(net.source_layer_add_ons.parameters(), net.extras.parameters(),\n","                             net.regression_headers.parameters(), net.classification_headers.parameters())\n","    params = [\n","        {'params': itertools.chain(\n","            net.source_layer_add_ons.parameters(),\n","            net.extras.parameters()\n","        ), 'lr': extra_layers_lr},\n","        {'params': itertools.chain(\n","            net.regression_headers.parameters(),\n","            net.classification_headers.parameters()\n","        )}\n","    ]\n","elif args.freeze_net:\n","    freeze_net_layers(net.base_net)\n","    freeze_net_layers(net.source_layer_add_ons)\n","    freeze_net_layers(net.extras)\n","    params = itertools.chain(net.regression_headers.parameters(), net.classification_headers.parameters())\n","    logging.info(\"Freeze all the layers except prediction heads.\")\n","else:\n","    params = [\n","        {'params': net.base_net.parameters(), 'lr': base_net_lr},\n","        {'params': itertools.chain(\n","            net.source_layer_add_ons.parameters(),\n","            net.extras.parameters()\n","        ), 'lr': extra_layers_lr},\n","        {'params': itertools.chain(\n","            net.regression_headers.parameters(),\n","            net.classification_headers.parameters()\n","        )}\n","    ]"]},{"cell_type":"code","execution_count":28,"id":"b94c27ea","metadata":{"execution":{"iopub.execute_input":"2023-04-28T15:38:38.016385Z","iopub.status.busy":"2023-04-28T15:38:38.016128Z","iopub.status.idle":"2023-04-29T01:49:50.636264Z","shell.execute_reply":"2023-04-29T01:49:50.633256Z"},"papermill":{"duration":36672.652266,"end_time":"2023-04-29T01:49:50.657878","exception":false,"start_time":"2023-04-28T15:38:38.005612","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 0.00 seconds to load the model.\n","Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: 0.001.\n","Uses MultiStepLR scheduler.\n","Start training from epoch 0.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Step: 90, Average Loss: 9.4685, Average Regression Loss 5.5461, Average Classification Loss: 3.9225\n","Epoch: 0, Validation Loss: 5.8540, Validation Regression Loss 2.2810, Validation Classification Loss: 3.5729\n","Epoch: 1, Step: 90, Average Loss: 9.4008, Average Regression Loss 5.5063, Average Classification Loss: 3.8945\n","Epoch: 2, Step: 90, Average Loss: 9.3043, Average Regression Loss 5.4335, Average Classification Loss: 3.8708\n","Epoch: 3, Step: 90, Average Loss: 9.3498, Average Regression Loss 5.4696, Average Classification Loss: 3.8802\n","Epoch: 4, Step: 90, Average Loss: 9.3874, Average Regression Loss 5.4699, Average Classification Loss: 3.9175\n","Epoch: 5, Step: 90, Average Loss: 9.3173, Average Regression Loss 5.4491, Average Classification Loss: 3.8683\n","Epoch: 6, Step: 90, Average Loss: 9.2936, Average Regression Loss 5.4031, Average Classification Loss: 3.8905\n","Epoch: 7, Step: 90, Average Loss: 9.2717, Average Regression Loss 5.4279, Average Classification Loss: 3.8438\n","Epoch: 8, Step: 90, Average Loss: 9.2855, Average Regression Loss 5.4308, Average Classification Loss: 3.8547\n","Epoch: 9, Step: 90, Average Loss: 9.4947, Average Regression Loss 5.6415, Average Classification Loss: 3.8531\n","Epoch: 10, Step: 90, Average Loss: 9.3028, Average Regression Loss 5.4714, Average Classification Loss: 3.8315\n","Epoch: 10, Validation Loss: 5.8212, Validation Regression Loss 2.2540, Validation Classification Loss: 3.5672\n","Epoch: 11, Step: 90, Average Loss: 9.2676, Average Regression Loss 5.4191, Average Classification Loss: 3.8485\n","Epoch: 12, Step: 90, Average Loss: 9.3070, Average Regression Loss 5.4794, Average Classification Loss: 3.8275\n","Epoch: 13, Step: 90, Average Loss: 9.0962, Average Regression Loss 5.2708, Average Classification Loss: 3.8254\n","Epoch: 14, Step: 90, Average Loss: 9.2047, Average Regression Loss 5.3850, Average Classification Loss: 3.8197\n","Epoch: 15, Step: 90, Average Loss: 9.0771, Average Regression Loss 5.2712, Average Classification Loss: 3.8059\n","Epoch: 16, Step: 90, Average Loss: 9.1886, Average Regression Loss 5.3372, Average Classification Loss: 3.8515\n","Epoch: 17, Step: 90, Average Loss: 9.2485, Average Regression Loss 5.4290, Average Classification Loss: 3.8195\n","Epoch: 18, Step: 90, Average Loss: 9.3575, Average Regression Loss 5.5573, Average Classification Loss: 3.8001\n","Epoch: 19, Step: 90, Average Loss: 9.1753, Average Regression Loss 5.3794, Average Classification Loss: 3.7959\n","Epoch: 20, Step: 90, Average Loss: 9.1484, Average Regression Loss 5.3464, Average Classification Loss: 3.8020\n","Epoch: 20, Validation Loss: 5.7309, Validation Regression Loss 2.2257, Validation Classification Loss: 3.5052\n","Epoch: 21, Step: 90, Average Loss: 9.2407, Average Regression Loss 5.4240, Average Classification Loss: 3.8167\n","Epoch: 22, Step: 90, Average Loss: 9.1903, Average Regression Loss 5.3944, Average Classification Loss: 3.7958\n","Epoch: 23, Step: 90, Average Loss: 9.2603, Average Regression Loss 5.4792, Average Classification Loss: 3.7811\n","Epoch: 24, Step: 90, Average Loss: 9.0752, Average Regression Loss 5.2910, Average Classification Loss: 3.7841\n","Epoch: 25, Step: 90, Average Loss: 9.1573, Average Regression Loss 5.3652, Average Classification Loss: 3.7921\n","Epoch: 26, Step: 90, Average Loss: 9.3477, Average Regression Loss 5.5570, Average Classification Loss: 3.7906\n","Epoch: 27, Step: 90, Average Loss: 9.1945, Average Regression Loss 5.4083, Average Classification Loss: 3.7862\n","Epoch: 28, Step: 90, Average Loss: 9.1922, Average Regression Loss 5.3993, Average Classification Loss: 3.7930\n","Epoch: 29, Step: 90, Average Loss: 9.2360, Average Regression Loss 5.4547, Average Classification Loss: 3.7813\n","Epoch: 30, Step: 90, Average Loss: 9.2742, Average Regression Loss 5.4773, Average Classification Loss: 3.7969\n","Epoch: 30, Validation Loss: 5.6890, Validation Regression Loss 2.2298, Validation Classification Loss: 3.4592\n","Epoch: 31, Step: 90, Average Loss: 9.2955, Average Regression Loss 5.5284, Average Classification Loss: 3.7671\n","Epoch: 32, Step: 90, Average Loss: 9.3329, Average Regression Loss 5.5597, Average Classification Loss: 3.7732\n","Epoch: 33, Step: 90, Average Loss: 9.2358, Average Regression Loss 5.4594, Average Classification Loss: 3.7764\n","Epoch: 34, Step: 90, Average Loss: 9.3215, Average Regression Loss 5.5617, Average Classification Loss: 3.7598\n","Epoch: 35, Step: 90, Average Loss: 9.1806, Average Regression Loss 5.4117, Average Classification Loss: 3.7689\n","Epoch: 36, Step: 90, Average Loss: 9.1798, Average Regression Loss 5.4114, Average Classification Loss: 3.7683\n","Epoch: 37, Step: 90, Average Loss: 9.0441, Average Regression Loss 5.2841, Average Classification Loss: 3.7600\n","Epoch: 38, Step: 90, Average Loss: 9.1389, Average Regression Loss 5.3703, Average Classification Loss: 3.7686\n","Epoch: 39, Step: 90, Average Loss: 9.1495, Average Regression Loss 5.3851, Average Classification Loss: 3.7644\n","Epoch: 40, Step: 90, Average Loss: 9.1366, Average Regression Loss 5.3986, Average Classification Loss: 3.7380\n","Epoch: 40, Validation Loss: 5.6774, Validation Regression Loss 2.2121, Validation Classification Loss: 3.4653\n","Epoch: 41, Step: 90, Average Loss: 9.1970, Average Regression Loss 5.4513, Average Classification Loss: 3.7457\n","Epoch: 42, Step: 90, Average Loss: 9.0189, Average Regression Loss 5.2792, Average Classification Loss: 3.7396\n","Epoch: 43, Step: 90, Average Loss: 9.3098, Average Regression Loss 5.5628, Average Classification Loss: 3.7471\n","Epoch: 44, Step: 90, Average Loss: 9.2029, Average Regression Loss 5.4391, Average Classification Loss: 3.7637\n","Epoch: 45, Step: 90, Average Loss: 9.2377, Average Regression Loss 5.4868, Average Classification Loss: 3.7509\n","Epoch: 46, Step: 90, Average Loss: 9.2331, Average Regression Loss 5.4548, Average Classification Loss: 3.7783\n","Epoch: 47, Step: 90, Average Loss: 9.1022, Average Regression Loss 5.3591, Average Classification Loss: 3.7431\n","Epoch: 48, Step: 90, Average Loss: 9.1352, Average Regression Loss 5.3952, Average Classification Loss: 3.7401\n","Epoch: 49, Step: 90, Average Loss: 9.1896, Average Regression Loss 5.4306, Average Classification Loss: 3.7590\n","Epoch: 50, Step: 90, Average Loss: 9.0736, Average Regression Loss 5.3539, Average Classification Loss: 3.7198\n","Epoch: 50, Validation Loss: 5.6526, Validation Regression Loss 2.2169, Validation Classification Loss: 3.4358\n","Epoch: 51, Step: 90, Average Loss: 9.1513, Average Regression Loss 5.4147, Average Classification Loss: 3.7366\n","Epoch: 52, Step: 90, Average Loss: 9.2730, Average Regression Loss 5.5312, Average Classification Loss: 3.7418\n","Epoch: 53, Step: 90, Average Loss: 9.0435, Average Regression Loss 5.3056, Average Classification Loss: 3.7378\n","Epoch: 54, Step: 90, Average Loss: 9.0831, Average Regression Loss 5.3513, Average Classification Loss: 3.7318\n","Epoch: 55, Step: 90, Average Loss: 9.1881, Average Regression Loss 5.4557, Average Classification Loss: 3.7325\n","Epoch: 56, Step: 90, Average Loss: 9.1718, Average Regression Loss 5.4440, Average Classification Loss: 3.7278\n","Epoch: 57, Step: 90, Average Loss: 9.2963, Average Regression Loss 5.5624, Average Classification Loss: 3.7339\n","Epoch: 58, Step: 90, Average Loss: 9.1330, Average Regression Loss 5.4044, Average Classification Loss: 3.7287\n","Epoch: 59, Step: 90, Average Loss: 9.0483, Average Regression Loss 5.3255, Average Classification Loss: 3.7228\n","Epoch: 60, Step: 90, Average Loss: 8.9964, Average Regression Loss 5.2707, Average Classification Loss: 3.7257\n","Epoch: 60, Validation Loss: 5.6149, Validation Regression Loss 2.2130, Validation Classification Loss: 3.4019\n","Epoch: 61, Step: 90, Average Loss: 9.2782, Average Regression Loss 5.5445, Average Classification Loss: 3.7337\n","Epoch: 62, Step: 90, Average Loss: 8.9436, Average Regression Loss 5.2149, Average Classification Loss: 3.7286\n","Epoch: 63, Step: 90, Average Loss: 9.1222, Average Regression Loss 5.4060, Average Classification Loss: 3.7161\n","Epoch: 64, Step: 90, Average Loss: 9.0977, Average Regression Loss 5.3620, Average Classification Loss: 3.7357\n","Epoch: 65, Step: 90, Average Loss: 9.1083, Average Regression Loss 5.3683, Average Classification Loss: 3.7401\n","Epoch: 66, Step: 90, Average Loss: 9.1201, Average Regression Loss 5.4169, Average Classification Loss: 3.7032\n","Epoch: 67, Step: 90, Average Loss: 9.0738, Average Regression Loss 5.3551, Average Classification Loss: 3.7188\n","Epoch: 68, Step: 90, Average Loss: 9.1263, Average Regression Loss 5.4071, Average Classification Loss: 3.7192\n","Epoch: 69, Step: 90, Average Loss: 9.2412, Average Regression Loss 5.5253, Average Classification Loss: 3.7159\n","Epoch: 70, Step: 90, Average Loss: 9.1046, Average Regression Loss 5.3652, Average Classification Loss: 3.7393\n","Epoch: 70, Validation Loss: 5.6131, Validation Regression Loss 2.2044, Validation Classification Loss: 3.4086\n","Epoch: 71, Step: 90, Average Loss: 9.0829, Average Regression Loss 5.3891, Average Classification Loss: 3.6937\n","Epoch: 72, Step: 90, Average Loss: 8.9912, Average Regression Loss 5.2916, Average Classification Loss: 3.6996\n","Epoch: 73, Step: 90, Average Loss: 9.0626, Average Regression Loss 5.3636, Average Classification Loss: 3.6990\n","Epoch: 74, Step: 90, Average Loss: 9.2578, Average Regression Loss 5.5507, Average Classification Loss: 3.7070\n","Epoch: 75, Step: 90, Average Loss: 9.0390, Average Regression Loss 5.3259, Average Classification Loss: 3.7131\n","Epoch: 76, Step: 90, Average Loss: 9.1587, Average Regression Loss 5.4773, Average Classification Loss: 3.6814\n","Epoch: 77, Step: 90, Average Loss: 9.2476, Average Regression Loss 5.5259, Average Classification Loss: 3.7216\n","Epoch: 78, Step: 90, Average Loss: 9.0772, Average Regression Loss 5.3739, Average Classification Loss: 3.7033\n","Epoch: 79, Step: 90, Average Loss: 8.9731, Average Regression Loss 5.3035, Average Classification Loss: 3.6696\n","Epoch: 80, Step: 90, Average Loss: 9.1006, Average Regression Loss 5.4272, Average Classification Loss: 3.6734\n","Epoch: 80, Validation Loss: 5.5258, Validation Regression Loss 2.1875, Validation Classification Loss: 3.3383\n","Epoch: 81, Step: 90, Average Loss: 9.1562, Average Regression Loss 5.4859, Average Classification Loss: 3.6703\n","Epoch: 82, Step: 90, Average Loss: 9.1317, Average Regression Loss 5.4795, Average Classification Loss: 3.6521\n","Epoch: 83, Step: 90, Average Loss: 9.2055, Average Regression Loss 5.5321, Average Classification Loss: 3.6734\n","Epoch: 84, Step: 90, Average Loss: 9.0630, Average Regression Loss 5.4105, Average Classification Loss: 3.6525\n","Epoch: 85, Step: 90, Average Loss: 8.9949, Average Regression Loss 5.3182, Average Classification Loss: 3.6768\n","Epoch: 86, Step: 90, Average Loss: 9.0380, Average Regression Loss 5.3664, Average Classification Loss: 3.6716\n","Epoch: 87, Step: 90, Average Loss: 8.8287, Average Regression Loss 5.1499, Average Classification Loss: 3.6788\n","Epoch: 88, Step: 90, Average Loss: 9.0575, Average Regression Loss 5.4172, Average Classification Loss: 3.6403\n","Epoch: 89, Step: 90, Average Loss: 8.9920, Average Regression Loss 5.3502, Average Classification Loss: 3.6417\n","Epoch: 90, Step: 90, Average Loss: 8.9782, Average Regression Loss 5.3403, Average Classification Loss: 3.6379\n","Epoch: 90, Validation Loss: 5.5001, Validation Regression Loss 2.1681, Validation Classification Loss: 3.3320\n","Epoch: 91, Step: 90, Average Loss: 9.0505, Average Regression Loss 5.3931, Average Classification Loss: 3.6574\n","Epoch: 92, Step: 90, Average Loss: 9.1739, Average Regression Loss 5.5141, Average Classification Loss: 3.6597\n","Epoch: 93, Step: 90, Average Loss: 9.0262, Average Regression Loss 5.3816, Average Classification Loss: 3.6446\n","Epoch: 94, Step: 90, Average Loss: 8.9918, Average Regression Loss 5.3344, Average Classification Loss: 3.6574\n","Epoch: 95, Step: 90, Average Loss: 9.1032, Average Regression Loss 5.4616, Average Classification Loss: 3.6416\n","Epoch: 96, Step: 90, Average Loss: 8.9229, Average Regression Loss 5.2670, Average Classification Loss: 3.6559\n","Epoch: 97, Step: 90, Average Loss: 9.1945, Average Regression Loss 5.5244, Average Classification Loss: 3.6700\n","Epoch: 98, Step: 90, Average Loss: 8.9783, Average Regression Loss 5.3292, Average Classification Loss: 3.6491\n","Epoch: 99, Step: 90, Average Loss: 9.0686, Average Regression Loss 5.4051, Average Classification Loss: 3.6635\n","Epoch: 100, Step: 90, Average Loss: 8.9457, Average Regression Loss 5.2969, Average Classification Loss: 3.6488\n","Epoch: 100, Validation Loss: 5.4997, Validation Regression Loss 2.1726, Validation Classification Loss: 3.3270\n","Epoch: 101, Step: 90, Average Loss: 9.1088, Average Regression Loss 5.4493, Average Classification Loss: 3.6595\n","Epoch: 102, Step: 90, Average Loss: 9.0134, Average Regression Loss 5.3625, Average Classification Loss: 3.6509\n","Epoch: 103, Step: 90, Average Loss: 9.0474, Average Regression Loss 5.4088, Average Classification Loss: 3.6386\n","Epoch: 104, Step: 90, Average Loss: 9.0807, Average Regression Loss 5.4384, Average Classification Loss: 3.6423\n","Epoch: 105, Step: 90, Average Loss: 9.2456, Average Regression Loss 5.6137, Average Classification Loss: 3.6319\n","Epoch: 106, Step: 90, Average Loss: 8.8935, Average Regression Loss 5.2443, Average Classification Loss: 3.6492\n","Epoch: 107, Step: 90, Average Loss: 9.0137, Average Regression Loss 5.3636, Average Classification Loss: 3.6501\n","Epoch: 108, Step: 90, Average Loss: 9.0267, Average Regression Loss 5.3635, Average Classification Loss: 3.6632\n","Epoch: 109, Step: 90, Average Loss: 8.9365, Average Regression Loss 5.2843, Average Classification Loss: 3.6522\n","Epoch: 110, Step: 90, Average Loss: 9.0548, Average Regression Loss 5.4036, Average Classification Loss: 3.6512\n","Epoch: 110, Validation Loss: 5.4975, Validation Regression Loss 2.1703, Validation Classification Loss: 3.3272\n","Epoch: 111, Step: 90, Average Loss: 8.8827, Average Regression Loss 5.2456, Average Classification Loss: 3.6370\n","Epoch: 112, Step: 90, Average Loss: 9.1079, Average Regression Loss 5.4691, Average Classification Loss: 3.6389\n","Epoch: 113, Step: 90, Average Loss: 8.8154, Average Regression Loss 5.1775, Average Classification Loss: 3.6380\n","Epoch: 114, Step: 90, Average Loss: 9.0201, Average Regression Loss 5.3560, Average Classification Loss: 3.6641\n","Epoch: 115, Step: 90, Average Loss: 8.9903, Average Regression Loss 5.3484, Average Classification Loss: 3.6418\n","Epoch: 116, Step: 90, Average Loss: 8.9052, Average Regression Loss 5.2284, Average Classification Loss: 3.6768\n","Epoch: 117, Step: 90, Average Loss: 9.0887, Average Regression Loss 5.4590, Average Classification Loss: 3.6296\n","Epoch: 118, Step: 90, Average Loss: 8.8467, Average Regression Loss 5.2077, Average Classification Loss: 3.6389\n","Epoch: 119, Step: 90, Average Loss: 8.9460, Average Regression Loss 5.2985, Average Classification Loss: 3.6475\n","Epoch: 120, Step: 90, Average Loss: 9.0868, Average Regression Loss 5.4320, Average Classification Loss: 3.6548\n","Epoch: 120, Validation Loss: 5.4940, Validation Regression Loss 2.1703, Validation Classification Loss: 3.3237\n","Epoch: 121, Step: 90, Average Loss: 8.9315, Average Regression Loss 5.2852, Average Classification Loss: 3.6463\n","Epoch: 122, Step: 90, Average Loss: 8.8141, Average Regression Loss 5.1561, Average Classification Loss: 3.6579\n","Epoch: 123, Step: 90, Average Loss: 8.8860, Average Regression Loss 5.2387, Average Classification Loss: 3.6473\n","Epoch: 124, Step: 90, Average Loss: 9.0110, Average Regression Loss 5.3689, Average Classification Loss: 3.6421\n","Epoch: 125, Step: 90, Average Loss: 9.1804, Average Regression Loss 5.5312, Average Classification Loss: 3.6492\n","Epoch: 126, Step: 90, Average Loss: 8.8715, Average Regression Loss 5.2113, Average Classification Loss: 3.6603\n","Epoch: 127, Step: 90, Average Loss: 9.1751, Average Regression Loss 5.5384, Average Classification Loss: 3.6367\n","Epoch: 128, Step: 90, Average Loss: 8.9925, Average Regression Loss 5.3283, Average Classification Loss: 3.6643\n","Epoch: 129, Step: 90, Average Loss: 9.0241, Average Regression Loss 5.3733, Average Classification Loss: 3.6508\n","Epoch: 130, Step: 90, Average Loss: 8.9957, Average Regression Loss 5.3146, Average Classification Loss: 3.6811\n","Epoch: 130, Validation Loss: 5.4921, Validation Regression Loss 2.1662, Validation Classification Loss: 3.3258\n","Epoch: 131, Step: 90, Average Loss: 9.0811, Average Regression Loss 5.4410, Average Classification Loss: 3.6401\n","Epoch: 132, Step: 90, Average Loss: 8.9278, Average Regression Loss 5.2576, Average Classification Loss: 3.6702\n","Epoch: 133, Step: 90, Average Loss: 8.9835, Average Regression Loss 5.3534, Average Classification Loss: 3.6301\n","Epoch: 134, Step: 90, Average Loss: 9.0129, Average Regression Loss 5.3581, Average Classification Loss: 3.6548\n","Epoch: 135, Step: 90, Average Loss: 8.9787, Average Regression Loss 5.3331, Average Classification Loss: 3.6456\n","Epoch: 136, Step: 90, Average Loss: 8.9980, Average Regression Loss 5.3409, Average Classification Loss: 3.6571\n","Epoch: 137, Step: 90, Average Loss: 8.9648, Average Regression Loss 5.3100, Average Classification Loss: 3.6548\n","Epoch: 138, Step: 90, Average Loss: 8.9642, Average Regression Loss 5.3208, Average Classification Loss: 3.6433\n","Epoch: 139, Step: 90, Average Loss: 8.9932, Average Regression Loss 5.3432, Average Classification Loss: 3.6500\n","Epoch: 140, Step: 90, Average Loss: 8.7471, Average Regression Loss 5.1241, Average Classification Loss: 3.6231\n","Epoch: 140, Validation Loss: 5.4940, Validation Regression Loss 2.1664, Validation Classification Loss: 3.3276\n","Epoch: 141, Step: 90, Average Loss: 9.0933, Average Regression Loss 5.4446, Average Classification Loss: 3.6486\n","Epoch: 142, Step: 90, Average Loss: 9.0380, Average Regression Loss 5.3847, Average Classification Loss: 3.6532\n","Epoch: 143, Step: 90, Average Loss: 8.9851, Average Regression Loss 5.3140, Average Classification Loss: 3.6711\n","Epoch: 144, Step: 90, Average Loss: 9.1006, Average Regression Loss 5.4596, Average Classification Loss: 3.6409\n","Epoch: 145, Step: 90, Average Loss: 8.8447, Average Regression Loss 5.1993, Average Classification Loss: 3.6453\n","Epoch: 146, Step: 90, Average Loss: 8.9443, Average Regression Loss 5.3033, Average Classification Loss: 3.6410\n","Epoch: 147, Step: 90, Average Loss: 8.8387, Average Regression Loss 5.2036, Average Classification Loss: 3.6352\n","Epoch: 148, Step: 90, Average Loss: 9.1116, Average Regression Loss 5.4646, Average Classification Loss: 3.6470\n","Epoch: 149, Step: 90, Average Loss: 8.9711, Average Regression Loss 5.2938, Average Classification Loss: 3.6773\n","Epoch: 150, Step: 90, Average Loss: 9.0950, Average Regression Loss 5.4429, Average Classification Loss: 3.6521\n","Epoch: 150, Validation Loss: 5.4970, Validation Regression Loss 2.1722, Validation Classification Loss: 3.3248\n","Epoch: 151, Step: 90, Average Loss: 8.9911, Average Regression Loss 5.3223, Average Classification Loss: 3.6687\n","Epoch: 152, Step: 90, Average Loss: 9.0153, Average Regression Loss 5.3441, Average Classification Loss: 3.6711\n","Epoch: 153, Step: 90, Average Loss: 9.0247, Average Regression Loss 5.3738, Average Classification Loss: 3.6508\n","Epoch: 154, Step: 90, Average Loss: 8.9420, Average Regression Loss 5.2965, Average Classification Loss: 3.6455\n","Epoch: 155, Step: 90, Average Loss: 9.0159, Average Regression Loss 5.3649, Average Classification Loss: 3.6510\n","Epoch: 156, Step: 90, Average Loss: 9.0698, Average Regression Loss 5.3950, Average Classification Loss: 3.6749\n","Epoch: 157, Step: 90, Average Loss: 8.9448, Average Regression Loss 5.2937, Average Classification Loss: 3.6511\n","Epoch: 158, Step: 90, Average Loss: 8.9486, Average Regression Loss 5.3191, Average Classification Loss: 3.6295\n","Epoch: 159, Step: 90, Average Loss: 8.9777, Average Regression Loss 5.3458, Average Classification Loss: 3.6320\n","Epoch: 160, Step: 90, Average Loss: 8.8603, Average Regression Loss 5.2267, Average Classification Loss: 3.6336\n","Epoch: 160, Validation Loss: 5.4965, Validation Regression Loss 2.1674, Validation Classification Loss: 3.3290\n","Epoch: 161, Step: 90, Average Loss: 9.0232, Average Regression Loss 5.3541, Average Classification Loss: 3.6691\n","Epoch: 162, Step: 90, Average Loss: 9.0644, Average Regression Loss 5.4233, Average Classification Loss: 3.6410\n","Epoch: 163, Step: 90, Average Loss: 9.0509, Average Regression Loss 5.4081, Average Classification Loss: 3.6427\n","Epoch: 164, Step: 90, Average Loss: 9.1004, Average Regression Loss 5.4662, Average Classification Loss: 3.6342\n","Epoch: 165, Step: 90, Average Loss: 8.8870, Average Regression Loss 5.2306, Average Classification Loss: 3.6564\n","Epoch: 166, Step: 90, Average Loss: 8.9637, Average Regression Loss 5.3080, Average Classification Loss: 3.6557\n","Epoch: 167, Step: 90, Average Loss: 9.0385, Average Regression Loss 5.3790, Average Classification Loss: 3.6595\n","Epoch: 168, Step: 90, Average Loss: 9.0392, Average Regression Loss 5.3754, Average Classification Loss: 3.6638\n","Epoch: 169, Step: 90, Average Loss: 8.9742, Average Regression Loss 5.3483, Average Classification Loss: 3.6259\n","Epoch: 170, Step: 90, Average Loss: 9.0640, Average Regression Loss 5.4231, Average Classification Loss: 3.6409\n","Epoch: 170, Validation Loss: 5.4909, Validation Regression Loss 2.1632, Validation Classification Loss: 3.3277\n","Epoch: 171, Step: 90, Average Loss: 8.9324, Average Regression Loss 5.2873, Average Classification Loss: 3.6451\n","Epoch: 172, Step: 90, Average Loss: 9.1183, Average Regression Loss 5.4734, Average Classification Loss: 3.6449\n","Epoch: 173, Step: 90, Average Loss: 9.1038, Average Regression Loss 5.4397, Average Classification Loss: 3.6641\n","Epoch: 174, Step: 90, Average Loss: 8.9245, Average Regression Loss 5.2839, Average Classification Loss: 3.6406\n","Epoch: 175, Step: 90, Average Loss: 8.9944, Average Regression Loss 5.3420, Average Classification Loss: 3.6525\n","Epoch: 176, Step: 90, Average Loss: 9.1011, Average Regression Loss 5.4392, Average Classification Loss: 3.6619\n","Epoch: 177, Step: 90, Average Loss: 9.0130, Average Regression Loss 5.3613, Average Classification Loss: 3.6516\n","Epoch: 178, Step: 90, Average Loss: 9.0049, Average Regression Loss 5.3688, Average Classification Loss: 3.6360\n","Epoch: 179, Step: 90, Average Loss: 9.0889, Average Regression Loss 5.4349, Average Classification Loss: 3.6540\n","Epoch: 180, Step: 90, Average Loss: 8.9737, Average Regression Loss 5.3448, Average Classification Loss: 3.6289\n","Epoch: 180, Validation Loss: 5.4950, Validation Regression Loss 2.1696, Validation Classification Loss: 3.3254\n","Epoch: 181, Step: 90, Average Loss: 8.9973, Average Regression Loss 5.3470, Average Classification Loss: 3.6504\n","Epoch: 182, Step: 90, Average Loss: 8.9282, Average Regression Loss 5.2811, Average Classification Loss: 3.6470\n","Epoch: 183, Step: 90, Average Loss: 8.9118, Average Regression Loss 5.2670, Average Classification Loss: 3.6448\n","Epoch: 184, Step: 90, Average Loss: 8.8829, Average Regression Loss 5.2600, Average Classification Loss: 3.6229\n","Epoch: 185, Step: 90, Average Loss: 9.0789, Average Regression Loss 5.4047, Average Classification Loss: 3.6742\n","Epoch: 186, Step: 90, Average Loss: 8.9744, Average Regression Loss 5.3263, Average Classification Loss: 3.6480\n","Epoch: 187, Step: 90, Average Loss: 8.9087, Average Regression Loss 5.2524, Average Classification Loss: 3.6564\n","Epoch: 188, Step: 90, Average Loss: 8.9642, Average Regression Loss 5.3174, Average Classification Loss: 3.6468\n","Epoch: 189, Step: 90, Average Loss: 9.0175, Average Regression Loss 5.3510, Average Classification Loss: 3.6665\n","Epoch: 190, Step: 90, Average Loss: 9.0510, Average Regression Loss 5.3916, Average Classification Loss: 3.6594\n","Epoch: 190, Validation Loss: 5.4910, Validation Regression Loss 2.1666, Validation Classification Loss: 3.3244\n","Epoch: 191, Step: 90, Average Loss: 8.8405, Average Regression Loss 5.2022, Average Classification Loss: 3.6383\n","Epoch: 192, Step: 90, Average Loss: 8.9655, Average Regression Loss 5.3214, Average Classification Loss: 3.6441\n","Epoch: 193, Step: 90, Average Loss: 8.8282, Average Regression Loss 5.1925, Average Classification Loss: 3.6357\n","Epoch: 194, Step: 90, Average Loss: 8.9741, Average Regression Loss 5.3275, Average Classification Loss: 3.6466\n","Epoch: 195, Step: 90, Average Loss: 8.8511, Average Regression Loss 5.2103, Average Classification Loss: 3.6408\n","Epoch: 196, Step: 90, Average Loss: 9.0031, Average Regression Loss 5.3623, Average Classification Loss: 3.6408\n","Epoch: 197, Step: 90, Average Loss: 8.8815, Average Regression Loss 5.2328, Average Classification Loss: 3.6487\n","Epoch: 198, Step: 90, Average Loss: 8.9479, Average Regression Loss 5.3090, Average Classification Loss: 3.6390\n","Epoch: 199, Step: 90, Average Loss: 9.0610, Average Regression Loss 5.4165, Average Classification Loss: 3.6445\n","Epoch: 199, Validation Loss: 5.4936, Validation Regression Loss 2.1695, Validation Classification Loss: 3.3241\n"]}],"source":["timer = Timer()\n","timer.start(\"Load Model\")\n","if args.resume:\n","    print(f\"Resume from the model {args.resume}\")\n","    net.load(args.resume)\n","elif args.base_net:\n","    print(f\"Init from base net {args.base_net}\")\n","    net.init_from_base_net(args.base_net)\n","elif args.pretrained_ssd:\n","    print(f\"Init from pretrained ssd {args.pretrained_ssd}\")\n","    net.init_from_pretrained_ssd(args.pretrained_ssd)\n","print(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n","\n","net.load('/kaggle/input/torch-ssd/mb1-ssd-Epoch-90-Loss-5.659898058398739.pth')\n","net.to(DEVICE)\n","\n","optimizer = torch.optim.SGD(params, lr=args.lr, momentum=args.momentum,\n","                            weight_decay=args.weight_decay)\n","criterion = MultiboxLoss(priors, iou_threshold=0.5, neg_pos_ratio=3,\n","                         center_variance=0.1, size_variance=0.2, device=DEVICE)\n","\n","print(f\"Learning rate: {args.lr}, Base net learning rate: {base_net_lr}, \"\n","             + f\"Extra Layers learning rate: {extra_layers_lr}.\")\n","\n","if args.scheduler == 'multi-step':\n","    print(\"Uses MultiStepLR scheduler.\")\n","    milestones = [int(v.strip()) for v in args.milestones.split(\",\")]\n","    scheduler = MultiStepLR(optimizer, milestones=milestones,\n","                                                 gamma=0.1, last_epoch=last_epoch)\n","elif args.scheduler == 'cosine':\n","    print(\"Uses CosineAnnealingLR scheduler.\")\n","    scheduler = CosineAnnealingLR(optimizer, args.t_max, last_epoch=last_epoch)\n","else:\n","    logging.fatal(f\"Unsupported Scheduler: {args.scheduler}.\")\n","    parser.print_help(sys.stderr)\n","    sys.exit(1)\n","\n","print(f\"Start training from epoch {last_epoch + 1}.\")\n","for epoch in range(last_epoch + 1, args.num_epochs):\n","    scheduler.step()\n","    train(train_loader, net, criterion, optimizer,\n","          device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)\n","\n","    if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:\n","        val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n","        print(\n","            f\"Epoch: {epoch}, \" +\n","            f\"Validation Loss: {val_loss:.4f}, \" +\n","            f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n","            f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n","        )\n","        model_path = os.path.join('/kaggle/working/', f\"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n","        net.save(model_path)\n","        #print(f\"Saved model {model_path}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":36688.071292,"end_time":"2023-04-29T01:49:52.568982","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-04-28T15:38:24.49769","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nimport logging\nimport sys\nimport itertools\nimport numpy as np\nimport pathlib\nimport xml.etree.ElementTree as ET\nimport cv2\nimport torch\nimport collections\nimport math\nimport time\nimport types\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\nfrom typing import List,Tuple\nfrom torchvision import transforms\nfrom numpy import random\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:33.819388Z","iopub.execute_input":"2023-04-19T12:05:33.819899Z","iopub.status.idle":"2023-04-19T12:05:33.827415Z","shell.execute_reply.started":"2023-04-19T12:05:33.819854Z","shell.execute_reply":"2023-04-19T12:05:33.826271Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"image_size = 300\nimage_mean = np.array([127, 127, 127])  # RGB layout\nimage_std = 128.0\niou_threshold = 0.45\ncenter_variance = 0.1\nsize_variance = 0.2\nnum_classes = 21","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:33.829704Z","iopub.execute_input":"2023-04-19T12:05:33.830073Z","iopub.status.idle":"2023-04-19T12:05:33.838759Z","shell.execute_reply.started":"2023-04-19T12:05:33.830037Z","shell.execute_reply":"2023-04-19T12:05:33.837717Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"# **VOCDataset**","metadata":{}},{"cell_type":"code","source":"class VOCDataset:\n\n    def __init__(self, root, transform=None, target_transform=None, is_test=False, keep_difficult=False, label_file=None):\n        \"\"\"Dataset for VOC data.\n        Args:\n            root: the root of the VOC2007 or VOC2012 dataset, the directory contains the following sub-directories:\n                Annotations, ImageSets, JPEGImages, SegmentationClass, SegmentationObject.\n        \"\"\"\n        self.root = pathlib.Path(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        if is_test:\n            image_sets_file = self.root / \"ImageSets/Main/val.txt\"\n        else:\n            image_sets_file = self.root / \"ImageSets/Main/trainval.txt\"\n        self.ids = VOCDataset._read_image_ids(image_sets_file)\n        self.keep_difficult = keep_difficult\n\n        # if the labels file exists, read in the class names\n        label_file_name = self.root / \"labels.txt\"\n\n        if os.path.isfile(label_file_name):\n            class_string = \"\"\n            with open(label_file_name, 'r') as infile:\n                for line in infile:\n                    class_string += line.rstrip()\n\n            # classes should be a comma separated list\n            \n            classes = class_string.split(',')\n            # prepend BACKGROUND as first class\n            classes.insert(0, 'BACKGROUND')\n            classes  = [ elem.replace(\" \", \"\") for elem in classes]\n            self.class_names = tuple(classes)\n            logging.info(\"VOC Labels read from file: \" + str(self.class_names))\n\n        else:\n            logging.info(\"No labels file, using default VOC classes.\")\n            self.class_names = ('BACKGROUND',\n            'aeroplane', 'bicycle', 'bird', 'boat',\n            'bottle', 'bus', 'car', 'cat', 'chair',\n            'cow', 'diningtable', 'dog', 'horse',\n            'motorbike', 'person', 'pottedplant',\n            'sheep', 'sofa', 'train', 'tvmonitor')\n\n\n        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n\n    def __getitem__(self, index):\n        image_id = self.ids[index]\n        boxes, labels, is_difficult = self._get_annotation(image_id)\n        if not self.keep_difficult:\n            boxes = boxes[is_difficult == 0]\n            labels = labels[is_difficult == 0]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        return image, boxes, labels\n\n    def get_image(self, index):\n        image_id = self.ids[index]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, _ = self.transform(image)\n        return image\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    @staticmethod\n    def _read_image_ids(image_sets_file):\n        ids = []\n        with open(image_sets_file) as f:\n            for line in f:\n                ids.append(line.rstrip())\n        return ids\n\n    def _get_annotation(self, image_id):\n        annotation_file = self.root / f\"Annotations/{image_id}.xml\"\n        objects = ET.parse(annotation_file).findall(\"object\")\n        boxes = []\n        labels = []\n        is_difficult = []\n        for object in objects:\n            class_name = object.find('name').text.lower().strip()\n            # we're only concerned with clases in our list\n            if class_name in self.class_dict:\n                bbox = object.find('bndbox')\n\n                # VOC dataset format follows Matlab, in which indexes start from 0\n                x1 = float(bbox.find('xmin').text) - 1\n                y1 = float(bbox.find('ymin').text) - 1\n                x2 = float(bbox.find('xmax').text) - 1\n                y2 = float(bbox.find('ymax').text) - 1\n                boxes.append([x1, y1, x2, y2])\n\n                labels.append(self.class_dict[class_name])\n                is_difficult_str = object.find('difficult').text\n                is_difficult.append(int(is_difficult_str) if is_difficult_str else 0)\n        \n        # for i in boxes:\n        #     i = np.array(i, dtype=np.float32)\n        # for j in labels:\n        #     j = np.array(j, dtype=np.int64)\n\n        return (np.array(boxes, dtype=np.float32),\n                np.array(labels, dtype=np.int64),\n                np.array(is_difficult, dtype=np.uint8))\n\n    def _read_image(self, image_id):\n        image_file = self.root / f\"JPEGImages/{image_id}.jpg\"\n        image = cv2.imread(str(image_file))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:33.840648Z","iopub.execute_input":"2023-04-19T12:05:33.841079Z","iopub.status.idle":"2023-04-19T12:05:33.864056Z","shell.execute_reply.started":"2023-04-19T12:05:33.841034Z","shell.execute_reply":"2023-04-19T12:05:33.863028Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"# **transforms**","metadata":{}},{"cell_type":"code","source":"def intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    \"\"\"Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    \"\"\"Applies a lambda as a transform.\"\"\"\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=image_size):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                 self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current, transform):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == 'BGR' and self.transform == 'HSV':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == 'RGB' and self.transform == 'HSV':\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif self.current == 'BGR' and self.transform == 'RGB':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif self.current == 'HSV' and self.transform == 'BGR':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        elif self.current == 'HSV' and self.transform == \"RGB\":\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    \"\"\"Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    \"\"\"\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float('-inf')\n            if max_iou is None:\n                max_iou = float('inf')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                              :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop's\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    \"\"\"Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    \"\"\"\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        \"\"\"\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        \"\"\"\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),  # RGB\n            ConvertColor(current=\"RGB\", transform='HSV'),  # HSV\n            RandomSaturation(),  # HSV\n            RandomHue(),  # HSV\n            ConvertColor(current='HSV', transform='RGB'),  # RGB\n            RandomContrast()  # RGB\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:33.953269Z","iopub.execute_input":"2023-04-19T12:05:33.953559Z","iopub.status.idle":"2023-04-19T12:05:34.006946Z","shell.execute_reply.started":"2023-04-19T12:05:33.953533Z","shell.execute_reply":"2023-04-19T12:05:34.006019Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"SSDBoxSizes = collections.namedtuple('SSDBoxSizes', ['min', 'max'])\nSSDSpec = collections.namedtuple('SSDSpec', ['feature_map_size', 'shrinkage', 'box_sizes', 'aspect_ratios'])\nspecs = [\n    SSDSpec(19, 16, SSDBoxSizes(60, 105), [2, 3]),\n    SSDSpec(10, 32, SSDBoxSizes(105, 150), [2, 3]),\n    SSDSpec(5, 64, SSDBoxSizes(150, 195), [2, 3]),\n    SSDSpec(3, 100, SSDBoxSizes(195, 240), [2, 3]),\n    SSDSpec(2, 150, SSDBoxSizes(240, 285), [2, 3]),\n    SSDSpec(1, 300, SSDBoxSizes(285, 330), [2, 3])\n]\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.009671Z","iopub.execute_input":"2023-04-19T12:05:34.010421Z","iopub.status.idle":"2023-04-19T12:05:34.020029Z","shell.execute_reply.started":"2023-04-19T12:05:34.010384Z","shell.execute_reply":"2023-04-19T12:05:34.019063Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"# **box_utils**","metadata":{}},{"cell_type":"code","source":"def generate_ssd_priors(specs: List[SSDSpec], image_size, clamp=True) -> torch.Tensor:\n    \"\"\"Generate SSD Prior Boxes.\n\n    It returns the center, height and width of the priors. The values are relative to the image size\n    Args:\n        specs: SSDSpecs about the shapes of sizes of prior boxes. i.e.\n            specs = [\n                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n            ]\n        image_size: image size.\n        clamp: if true, clamp the values to make fall between [0.0, 1.0]\n    Returns:\n        priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n            are relative to the image size.\n    \"\"\"\n    priors = []\n    for spec in specs:\n        scale = image_size / spec.shrinkage\n        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n            x_center = (i + 0.5) / scale\n            y_center = (j + 0.5) / scale\n\n            # small sized square box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # big sized square box\n            size = math.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # change h/w ratio of the small sized box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            for ratio in spec.aspect_ratios:\n                ratio = math.sqrt(ratio)\n                priors.append([\n                    x_center,\n                    y_center,\n                    w * ratio,\n                    h / ratio\n                ])\n                priors.append([\n                    x_center,\n                    y_center,\n                    w / ratio,\n                    h * ratio\n                ])\n\n    priors = torch.tensor(priors)\n    if clamp:\n        torch.clamp(priors, 0.0, 1.0, out=priors)\n    return priors\ndef convert_locations_to_boxes(locations, priors, center_variance,\n                               size_variance):\n    \"\"\"Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n\n    The conversion:\n        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n    We do it in the inverse direction here.\n    Args:\n        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n        center_variance: a float used to change the scale of center.\n        size_variance: a float used to change of scale of size.\n    Returns:\n        boxes:  priors: [[center_x, center_y, h, w]]. All the values\n            are relative to the image size.\n    \"\"\"\n    # priors can have one dimension less.\n    if priors.dim() + 1 == locations.dim():\n        priors = priors.unsqueeze(0)\n    return torch.cat([\n        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n        torch.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n    ], dim=locations.dim() - 1)\n\n\ndef convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n    # priors can have one dimension less\n    if center_form_priors.dim() + 1 == center_form_boxes.dim():\n        center_form_priors = center_form_priors.unsqueeze(0)\n    return torch.cat([\n        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n        torch.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n    ], dim=center_form_boxes.dim() - 1)\n\n\ndef area_of(left_top, right_bottom) -> torch.Tensor:\n    \"\"\"Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    \"\"\"\n    hw = torch.clamp(right_bottom - left_top, min=0.0)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    \"\"\"\n    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef assign_priors(gt_boxes, gt_labels, corner_form_priors,\n                  iou_threshold):\n    \"\"\"Assign ground truth boxes and targets to priors.\n\n    Args:\n        gt_boxes (num_targets, 4): ground truth boxes.\n        gt_labels (num_targets): labels of targets.\n        priors (num_priors, 4): corner form priors\n    Returns:\n        boxes (num_priors, 4): real values for priors.\n        labels (num_priros): labels for priors.\n    \"\"\"\n    # size: num_priors x num_targets\n    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_priors.unsqueeze(1))\n    # size: num_priors\n    best_target_per_prior, best_target_per_prior_index = ious.max(1)\n    # size: num_targets\n    best_prior_per_target, best_prior_per_target_index = ious.max(0)\n\n    for target_index, prior_index in enumerate(best_prior_per_target_index):\n        best_target_per_prior_index[prior_index] = target_index\n    # 2.0 is used to make sure every target has a prior assigned\n    best_target_per_prior.index_fill_(0, best_prior_per_target_index, 2)\n    # size: num_priors\n    labels = gt_labels[best_target_per_prior_index]\n    labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n    boxes = gt_boxes[best_target_per_prior_index]\n    return boxes, labels\n\n\ndef hard_negative_mining(loss, labels, neg_pos_ratio):\n    \"\"\"\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    \"\"\"\n    pos_mask = labels > 0\n    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n    num_neg = num_pos * neg_pos_ratio\n\n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=1, descending=True)\n    _, orders = indexes.sort(dim=1)\n    neg_mask = orders < num_neg\n    return pos_mask | neg_mask\n\n\ndef center_form_to_corner_form(locations):\n    return torch.cat([locations[..., :2] - locations[..., 2:]/2,\n                     locations[..., :2] + locations[..., 2:]/2], locations.dim() - 1) \n\n\ndef corner_form_to_center_form(boxes):\n    return torch.cat([\n        (boxes[..., :2] + boxes[..., 2:]) / 2,\n         boxes[..., 2:] - boxes[..., :2]\n    ], boxes.dim() - 1)\n\n\ndef hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n    \"\"\"\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        iou_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    \"\"\"\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :-1]\n    picked = []\n    _, indexes = scores.sort(descending=True)\n    indexes = indexes[:candidate_size]\n    while len(indexes) > 0:\n        current = indexes[0]\n        picked.append(current.item())\n        if 0 < top_k == len(picked) or len(indexes) == 1:\n            break\n        current_box = boxes[current, :]\n        indexes = indexes[1:]\n        rest_boxes = boxes[indexes, :]\n        iou = iou_of(\n            rest_boxes,\n            current_box.unsqueeze(0),\n        )\n        indexes = indexes[iou <= iou_threshold]\n\n    return box_scores[picked, :]\n\n\ndef nms(box_scores, nms_method=None, score_threshold=None, iou_threshold=None,\n        sigma=0.5, top_k=-1, candidate_size=200):\n    if nms_method == \"soft\":\n        return soft_nms(box_scores, score_threshold, sigma, top_k)\n    else:\n        return hard_nms(box_scores, iou_threshold, top_k, candidate_size=candidate_size)\n\n\ndef soft_nms(box_scores, score_threshold, sigma=0.5, top_k=-1):\n    \"\"\"Soft NMS implementation.\n\n    References:\n        https://arxiv.org/abs/1704.04503\n        https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/cython_nms.pyx\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        score_threshold: boxes with scores less than value are not considered.\n        sigma: the parameter in score re-computation.\n            scores[i] = scores[i] * exp(-(iou_i)^2 / simga)\n        top_k: keep top_k results. If k <= 0, keep all the results.\n    Returns:\n         picked_box_scores (K, 5): results of NMS.\n    \"\"\"\n    picked_box_scores = []\n    while box_scores.size(0) > 0:\n        max_score_index = torch.argmax(box_scores[:, 4])\n        cur_box_prob = torch.tensor(box_scores[max_score_index, :])\n        picked_box_scores.append(cur_box_prob)\n        if len(picked_box_scores) == top_k > 0 or box_scores.size(0) == 1:\n            break\n        cur_box = cur_box_prob[:-1]\n        box_scores[max_score_index, :] = box_scores[-1, :]\n        box_scores = box_scores[:-1, :]\n        ious = iou_of(cur_box.unsqueeze(0), box_scores[:, :-1])\n        box_scores[:, -1] = box_scores[:, -1] * torch.exp(-(ious * ious) / sigma)\n        box_scores = box_scores[box_scores[:, -1] > score_threshold, :]\n    if len(picked_box_scores) > 0:\n        return torch.stack(picked_box_scores)\n    else:\n        return torch.tensor([])\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.023169Z","iopub.execute_input":"2023-04-19T12:05:34.023520Z","iopub.status.idle":"2023-04-19T12:05:34.056548Z","shell.execute_reply.started":"2023-04-19T12:05:34.023484Z","shell.execute_reply":"2023-04-19T12:05:34.055587Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"priors = generate_ssd_priors(specs, image_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.058842Z","iopub.execute_input":"2023-04-19T12:05:34.059929Z","iopub.status.idle":"2023-04-19T12:05:34.070908Z","shell.execute_reply.started":"2023-04-19T12:05:34.059893Z","shell.execute_reply":"2023-04-19T12:05:34.069970Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# **Predictor**","metadata":{}},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, net, size, mean=0.0, std=1.0, nms_method=None,\n                 iou_threshold=0.45, filter_threshold=0.01, candidate_size=200, sigma=0.5, device=None):\n        self.net = net\n        self.transform = PredictionTransform(size, mean, std)\n        self.iou_threshold = iou_threshold\n        self.filter_threshold = filter_threshold\n        self.candidate_size = candidate_size\n        self.nms_method = nms_method\n\n        self.sigma = sigma\n        if device:\n            self.device = device\n        else:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        self.net.to(self.device)\n        self.net.eval()\n\n        self.timer = Timer()\n\n    def predict(self, image, top_k=-1, prob_threshold=None):\n        cpu_device = torch.device(\"cpu\")\n        height, width, _ = image.shape\n        image = self.transform(image)\n        images = image.unsqueeze(0)\n        images = images.to(self.device)\n        with torch.no_grad():\n            self.timer.start()\n            scores, boxes = self.net.forward(images)\n            print(\"Inference time: \", self.timer.end())\n        boxes = boxes[0]\n        scores = scores[0]\n        if not prob_threshold:\n            prob_threshold = self.filter_threshold\n        # this version of nms is slower on GPU, so we move data to CPU.\n        boxes = boxes.to(cpu_device)\n        scores = scores.to(cpu_device)\n        picked_box_probs = []\n        picked_labels = []\n        for class_index in range(1, scores.size(1)):\n            probs = scores[:, class_index]\n            mask = probs > prob_threshold\n            probs = probs[mask]\n            if probs.size(0) == 0:\n                continue\n            subset_boxes = boxes[mask, :]\n            box_probs = torch.cat([subset_boxes, probs.reshape(-1, 1)], dim=1)\n            box_probs = nms(box_probs, self.nms_method,\n                                      score_threshold=prob_threshold,\n                                      iou_threshold=self.iou_threshold,\n                                      sigma=self.sigma,\n                                      top_k=top_k,\n                                      candidate_size=self.candidate_size)\n            picked_box_probs.append(box_probs)\n            picked_labels.extend([class_index] * box_probs.size(0))\n        if not picked_box_probs:\n            return torch.tensor([]), torch.tensor([]), torch.tensor([])\n        picked_box_probs = torch.cat(picked_box_probs)\n        picked_box_probs[:, 0] *= width\n        picked_box_probs[:, 1] *= height\n        picked_box_probs[:, 2] *= width\n        picked_box_probs[:, 3] *= height\n        return picked_box_probs[:, :4], torch.tensor(picked_labels), picked_box_probs[:, 4]","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.075207Z","iopub.execute_input":"2023-04-19T12:05:34.075494Z","iopub.status.idle":"2023-04-19T12:05:34.097040Z","shell.execute_reply.started":"2023-04-19T12:05:34.075458Z","shell.execute_reply":"2023-04-19T12:05:34.095886Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"# **target_transform**","metadata":{}},{"cell_type":"code","source":"class MatchPrior(object):\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = corner_form_to_center_form(boxes)\n        locations = convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n        return locations, labels","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.099509Z","iopub.execute_input":"2023-04-19T12:05:34.099773Z","iopub.status.idle":"2023-04-19T12:05:34.108761Z","shell.execute_reply.started":"2023-04-19T12:05:34.099749Z","shell.execute_reply":"2023-04-19T12:05:34.107750Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"# **train_transform**","metadata":{}},{"cell_type":"code","source":"class TrainAugmentation:\n    def __init__(self, size, mean=0, std=1.0):\n        \"\"\"\n        Args:\n            size: the size the of final image.\n            mean: mean pixel value per channel.\n        \"\"\"\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            \n            ConvertFromInts(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            # RandomSampleCrop(),\n            Resize(self.size),\n            RandomMirror(),\n            ToPercentCoords(),\n            SubtractMeans(self.mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor(),\n        ])\n\n    def __call__(self, img, boxes, labels):\n        \"\"\"\n\n        Args:\n            img: the output of cv.imread in RGB layout.\n            boxes: boundding boxes in the form of (x1, y1, x2, y2).\n            labels: labels of boxes.\n        \"\"\"\n        return self.augment(img, boxes, labels)\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.110319Z","iopub.execute_input":"2023-04-19T12:05:34.110811Z","iopub.status.idle":"2023-04-19T12:05:34.120962Z","shell.execute_reply.started":"2023-04-19T12:05:34.110775Z","shell.execute_reply":"2023-04-19T12:05:34.119906Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"# **test_transform**","metadata":{}},{"cell_type":"code","source":"class TestTransform:\n    def __init__(self, size, mean=0.0, std=1.0):\n        self.transform = Compose([\n            ToPercentCoords(),\n            Resize(size),\n            SubtractMeans(mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor(),\n        ])\n\n    def __call__(self, image, boxes, labels):\n        return self.transform(image, boxes, labels)\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.123941Z","iopub.execute_input":"2023-04-19T12:05:34.124364Z","iopub.status.idle":"2023-04-19T12:05:34.131669Z","shell.execute_reply.started":"2023-04-19T12:05:34.124322Z","shell.execute_reply":"2023-04-19T12:05:34.130511Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"# **PredictionTransform**","metadata":{}},{"cell_type":"code","source":"class PredictionTransform:\n    def __init__(self, size, mean=0.0, std=1.0):\n        self.transform = Compose([\n            Resize(size),\n            SubtractMeans(mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor()\n        ])\n\n    def __call__(self, image):\n        image, _, _ = self.transform(image)\n        return image","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.132965Z","iopub.execute_input":"2023-04-19T12:05:34.133924Z","iopub.status.idle":"2023-04-19T12:05:34.141284Z","shell.execute_reply.started":"2023-04-19T12:05:34.133885Z","shell.execute_reply":"2023-04-19T12:05:34.140137Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"# **misc**","metadata":{}},{"cell_type":"code","source":"def str2bool(s):\n    return s.lower() in ('true', '1')\n\n\nclass Timer:\n    def __init__(self):\n        self.clock = {}\n\n    def start(self, key=\"default\"):\n        self.clock[key] = time.time()\n\n    def end(self, key=\"default\"):\n        if key not in self.clock:\n            raise Exception(f\"{key} is not in the clock.\")\n        interval = time.time() - self.clock[key]\n        del self.clock[key]\n        return interval\n        \n\ndef save_checkpoint(epoch, net_state_dict, optimizer_state_dict, best_score, checkpoint_path, model_path):\n    torch.save({\n        'epoch': epoch,\n        'model': net_state_dict,\n        'optimizer': optimizer_state_dict,\n        'best_score': best_score\n    }, checkpoint_path)\n    torch.save(net_state_dict, model_path)\n        \n        \ndef load_checkpoint(checkpoint_path):\n    return torch.load(checkpoint_path)\n\n\ndef freeze_net_layers(net):\n    for param in net.parameters():\n        param.requires_grad = False\n\n\ndef store_labels(path, labels):\n    with open(path, \"w\") as f:\n        f.write(\"\\n\".join(labels))\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.142946Z","iopub.execute_input":"2023-04-19T12:05:34.143635Z","iopub.status.idle":"2023-04-19T12:05:34.153869Z","shell.execute_reply.started":"2023-04-19T12:05:34.143598Z","shell.execute_reply":"2023-04-19T12:05:34.152864Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(\n    description='Single Shot MultiBox Detector Training With Pytorch')\n\nparser.add_argument(\"--dataset_type\", default=\"voc\", type=str,\n                    help='Specify dataset type. Currently support voc and open_images.')\n\nparser.add_argument('--datasets', nargs='+', help='Dataset directory path')\nparser.add_argument('--validation_dataset', help='Dataset directory path')\nparser.add_argument('--balance_data', action='store_true',\n                    help=\"Balance training data by down-sampling more frequent labels.\")\n\n\nparser.add_argument('--net', default=\"mb1-lite-ssd\",\n                    help=\"The network architecture, it can be mb1-ssd, mb1-lite-ssd, mb2-ssd-lite, mb3-large-ssd-lite, mb3-small-ssd-lite or vgg16-ssd.\")\nparser.add_argument('--freeze_base_net', action='store_true',\n                    help=\"Freeze base net layers.\")\nparser.add_argument('--freeze_net', action='store_true',\n                    help=\"Freeze all the layers except the prediction head.\")\n\nparser.add_argument('--mb2_width_mult', default=1.0, type=float,\n                    help='Width Multiplifier for MobilenetV2')\n\n# Params for SGD\nparser.add_argument('--lr', '--learning-rate', default=1e-3, type=float,\n                    help='initial learning rate')\nparser.add_argument('--momentum', default=0.9, type=float,\n                    help='Momentum value for optim')\nparser.add_argument('--weight_decay', default=5e-4, type=float,\n                    help='Weight decay for SGD')\nparser.add_argument('--gamma', default=0.1, type=float,\n                    help='Gamma update for SGD')\nparser.add_argument('--base_net_lr', default=None, type=float,\n                    help='initial learning rate for base net.')\nparser.add_argument('--extra_layers_lr', default=None, type=float,\n                    help='initial learning rate for the layers not in base net and prediction heads.')\n\n\n# Params for loading pretrained basenet or checkpoints.\nparser.add_argument('--base_net',\n                    help='Pretrained base model')\nparser.add_argument('--pretrained_ssd', help='Pre-trained base model')\nparser.add_argument('--resume', default=None, type=str,\n                    help='Checkpoint state_dict file to resume training from')\n\n# Scheduler\nparser.add_argument('--scheduler', default=\"multi-step\", type=str,\n                    help=\"Scheduler for SGD. It can one of multi-step and cosine\")\n\n# Params for Multi-step Scheduler\nparser.add_argument('--milestones', default=\"80,100\", type=str,\n                    help=\"milestones for MultiStepLR\")\n\n# Params for Cosine Annealing\nparser.add_argument('--t_max', default=100, type=float,\n                    help='T_max value for Cosine Annealing Scheduler.')\n\n# Train params\nparser.add_argument('--batch_size', default=100, type=int,\n                    help='Batch size for training')\nparser.add_argument('--num_epochs', default=50, type=int,\n                    help='the number epochs')\nparser.add_argument('--num_workers', default=4, type=int,\n                    help='Number of workers used in dataloading')\nparser.add_argument('--validation_epochs', default=100, type=int,\n                    help='the number epochs')\nparser.add_argument('--debug_steps', default=100, type=int,\n                    help='Set the debug log output frequency.')\nparser.add_argument('--use_cuda', default=True, type=str2bool,\n                    help='Use CUDA to train model')\n\nparser.add_argument('--checkpoint_folder', default='/kaggle/working/VOC2012/',\n                    help='Directory for saving checkpoint models')\n\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nargs = parser.parse_args(args=[])\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n\nif args.use_cuda and torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    print(\"Use Cuda.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.155713Z","iopub.execute_input":"2023-04-19T12:05:34.156457Z","iopub.status.idle":"2023-04-19T12:05:34.175541Z","shell.execute_reply.started":"2023-04-19T12:05:34.156420Z","shell.execute_reply":"2023-04-19T12:05:34.174321Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stdout","text":"Use Cuda.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/pascal-voc-2012/VOC2012'","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.177255Z","iopub.execute_input":"2023-04-19T12:05:34.177747Z","iopub.status.idle":"2023-04-19T12:05:34.186245Z","shell.execute_reply.started":"2023-04-19T12:05:34.177643Z","shell.execute_reply":"2023-04-19T12:05:34.185242Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"train_transform = TrainAugmentation(image_size, image_mean, image_std)\ntarget_transform = MatchPrior(priors, center_variance, size_variance, 0.5)\ntest_transform = TestTransform(image_size, image_mean, image_std)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.188228Z","iopub.execute_input":"2023-04-19T12:05:34.188647Z","iopub.status.idle":"2023-04-19T12:05:34.196078Z","shell.execute_reply.started":"2023-04-19T12:05:34.188555Z","shell.execute_reply":"2023-04-19T12:05:34.195018Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"# **train_loader**","metadata":{}},{"cell_type":"code","source":"train_dataset = VOCDataset(dataset_path, transform=train_transform,\n                             target_transform=target_transform)\n#train_dataset = ConcatDataset(dataset)\ntrain_loader = DataLoader(train_dataset, args.batch_size,\n                          num_workers=args.num_workers,\n                          shuffle=True)\nprint(\"Train dataset size: {}\".format(len(train_dataset)))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.200720Z","iopub.execute_input":"2023-04-19T12:05:34.201153Z","iopub.status.idle":"2023-04-19T12:05:34.214650Z","shell.execute_reply.started":"2023-04-19T12:05:34.201126Z","shell.execute_reply":"2023-04-19T12:05:34.214019Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"Train dataset size: 11540\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_dataset[10])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.215876Z","iopub.execute_input":"2023-04-19T12:05:34.216494Z","iopub.status.idle":"2023-04-19T12:05:34.244180Z","shell.execute_reply.started":"2023-04-19T12:05:34.216457Z","shell.execute_reply":"2023-04-19T12:05:34.243061Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"(tensor([[[-0.0211,  0.1741,  0.1763,  ..., -0.2661, -0.2094, -0.1874],\n         [ 0.0077,  0.1435,  0.2041,  ..., -0.2658, -0.2150, -0.1896],\n         [ 0.0461,  0.1203,  0.2568,  ..., -0.2811, -0.2243, -0.1989],\n         ...,\n         [-0.4403, -0.4314, -0.4286,  ..., -0.3496, -0.3004, -0.2574],\n         [-0.5100, -0.4871, -0.4636,  ..., -0.3171, -0.2772, -0.2914],\n         [-0.5725, -0.5282, -0.5122,  ..., -0.2809, -0.2298, -0.2797]],\n\n        [[ 0.0939,  0.2902,  0.2776,  ..., -0.2923, -0.3009, -0.2802],\n         [ 0.1054,  0.2345,  0.2897,  ..., -0.2893, -0.2858, -0.2745],\n         [ 0.1069,  0.1856,  0.3189,  ..., -0.2911, -0.2816, -0.2643],\n         ...,\n         [-0.4934, -0.4784, -0.4612,  ..., -0.4137, -0.3607, -0.3176],\n         [-0.5345, -0.5028, -0.4677,  ..., -0.3820, -0.3321, -0.3463],\n         [-0.5793, -0.5202, -0.4941,  ..., -0.3427, -0.2818, -0.3316]],\n\n        [[-0.0352,  0.2249,  0.2833,  ..., -0.4245, -0.4091, -0.3877],\n         [ 0.0065,  0.1957,  0.3181,  ..., -0.4224, -0.4026, -0.3859],\n         [ 0.0534,  0.1878,  0.3753,  ..., -0.4239, -0.4026, -0.3828],\n         ...,\n         [-0.5196, -0.5075, -0.4957,  ..., -0.4722, -0.4249, -0.3818],\n         [-0.5722, -0.5400, -0.5097,  ..., -0.4536, -0.4156, -0.4298],\n         [-0.6189, -0.5642, -0.5428,  ..., -0.4221, -0.3710, -0.4208]]]), tensor([[39.6667, 30.3333,  7.1356,  9.0687],\n        [29.9852, 22.9298,  5.7365,  7.6697],\n        [28.0486, 42.8978,  5.4027, 10.8016],\n        ...,\n        [ 4.7637,  1.3333,  1.0777,  1.0215],\n        [ 3.2000,  2.4309, -0.9116,  4.0245],\n        [ 5.8343,  1.3333,  2.0914,  1.0215]]), tensor([0, 0, 0,  ..., 0, 0, 0]))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **val_loader**","metadata":{}},{"cell_type":"code","source":"val_dataset = VOCDataset(dataset_path, transform=test_transform,\n                                 target_transform=target_transform, is_test=True)\nval_loader = DataLoader(val_dataset, args.batch_size,\n                        num_workers=args.num_workers,\n                        shuffle=False)\nprint(\"Test dataset size: {}\".format(len(val_dataset)))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.246073Z","iopub.execute_input":"2023-04-19T12:05:34.246711Z","iopub.status.idle":"2023-04-19T12:05:34.256432Z","shell.execute_reply.started":"2023-04-19T12:05:34.246672Z","shell.execute_reply":"2023-04-19T12:05:34.255254Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"Test dataset size: 5823\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **import**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import namedtuple\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU, BatchNorm2d","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.257741Z","iopub.execute_input":"2023-04-19T12:05:34.258581Z","iopub.status.idle":"2023-04-19T12:05:34.265165Z","shell.execute_reply.started":"2023-04-19T12:05:34.258544Z","shell.execute_reply":"2023-04-19T12:05:34.264064Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"# **MobileNet**","metadata":{}},{"cell_type":"code","source":"class MobileNetV1(nn.Module):\n    def __init__(self, num_classes=1024):\n        super(MobileNetV1, self).__init__()\n\n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n\n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(3, 32, 2),\n            conv_dw(32, 64, 1),\n            conv_dw(64, 128, 2),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1),\n        )\n        self.fc = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = F.avg_pool2d(x, 7)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.266682Z","iopub.execute_input":"2023-04-19T12:05:34.267832Z","iopub.status.idle":"2023-04-19T12:05:34.279825Z","shell.execute_reply.started":"2023-04-19T12:05:34.267796Z","shell.execute_reply":"2023-04-19T12:05:34.279048Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"# **SSD**","metadata":{}},{"cell_type":"code","source":"\nGraphPath = namedtuple(\"GraphPath\", ['s0', 'name', 's1'])  #\n\nclass SSD(nn.Module):\n    def __init__(self, num_classes: int, base_net: nn.ModuleList, source_layer_indexes: List[int],\n                 extras: nn.ModuleList, classification_headers: nn.ModuleList,\n                 regression_headers: nn.ModuleList, center_variance: nn.ModuleList, size_variance: nn.ModuleList, is_test=False, device=None):\n        \"\"\"Compose a SSD model using the given components.\n        \"\"\"\n        super(SSD, self).__init__()\n\n        self.num_classes = num_classes\n        self.base_net = base_net\n        self.source_layer_indexes = source_layer_indexes\n        self.extras = extras\n        self.classification_headers = classification_headers\n        self.regression_headers = regression_headers\n        self.is_test = is_test\n\n        # register layers in source_layer_indexes by adding them to a module list\n        self.source_layer_add_ons = nn.ModuleList([t[1] for t in source_layer_indexes\n                                                   if isinstance(t, tuple) and not isinstance(t, GraphPath)])\n        if device:\n            self.device = device\n        else:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        if is_test:\n            self.priors = priors.to(self.device)\n            self.center_variance = center_variance\n            self.size_variance = size_variance\n            \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        confidences = []\n        locations = []\n        start_layer_index = 0\n        header_index = 0\n        for end_layer_index in self.source_layer_indexes:\n            if isinstance(end_layer_index, GraphPath):\n                path = end_layer_index\n                end_layer_index = end_layer_index.s0\n                added_layer = None\n            elif isinstance(end_layer_index, tuple):\n                added_layer = end_layer_index[1]\n                end_layer_index = end_layer_index[0]\n                path = None\n            else:\n                added_layer = None\n                path = None\n            for layer in self.base_net[start_layer_index: end_layer_index]:\n                x = layer(x)\n            if added_layer:\n                y = added_layer(x)\n            else:\n                y = x\n            if path:\n                sub = getattr(self.base_net[end_layer_index], path.name)\n                for layer in sub[:path.s1]:\n                    x = layer(x)\n                y = x\n                for layer in sub[path.s1:]:\n                    x = layer(x)\n                end_layer_index += 1\n            start_layer_index = end_layer_index\n            confidence, location = self.compute_header(header_index, y)\n            header_index += 1\n            confidences.append(confidence)\n            locations.append(location)\n\n        for layer in self.base_net[end_layer_index:]:\n            x = layer(x)\n\n        for layer in self.extras:\n            x = layer(x)\n            confidence, location = self.compute_header(header_index, x)\n            header_index += 1\n            confidences.append(confidence)\n            locations.append(location)\n\n        confidences = torch.cat(confidences, 1)\n        locations = torch.cat(locations, 1)\n        \n        if self.is_test:\n            confidences = F.softmax(confidences, dim=2)\n            boxes = convert_locations_to_boxes(\n                locations, self.priors, self.center_variance, self.size_variance\n            )\n            boxes = center_form_to_corner_form(boxes)\n            return confidences, boxes\n        else:\n            return confidences, locations\n\n    def compute_header(self, i, x):\n        confidence = self.classification_headers[i](x)\n        confidence = confidence.permute(0, 2, 3, 1).contiguous()\n        confidence = confidence.view(confidence.size(0), -1, self.num_classes)\n\n        location = self.regression_headers[i](x)\n        location = location.permute(0, 2, 3, 1).contiguous()\n        location = location.view(location.size(0), -1, 4)\n\n        return confidence, location\n\n    def init_from_base_net(self, model):\n        self.base_net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage), strict=True)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def init_from_pretrained_ssd(self, model):\n        state_dict = torch.load(model, map_location=lambda storage, loc: storage)\n        state_dict = {k: v for k, v in state_dict.items() if not (k.startswith(\"classification_headers\") or k.startswith(\"regression_headers\"))}\n        model_dict = self.state_dict()\n        model_dict.update(state_dict)\n        self.load_state_dict(model_dict)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def init(self):\n        self.base_net.apply(_xavier_init_)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def load(self, model):\n        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n\n    def save(self, model_path):\n        torch.save(self.state_dict(), model_path)\n\n\nclass MatchPrior(object):\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = corner_form_to_center_form(boxes)\n        locations = convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n        return locations, labels\n\n\ndef _xavier_init_(m: nn.Module):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_uniform_(m.weight)\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.281381Z","iopub.execute_input":"2023-04-19T12:05:34.282086Z","iopub.status.idle":"2023-04-19T12:05:34.310146Z","shell.execute_reply.started":"2023-04-19T12:05:34.282046Z","shell.execute_reply":"2023-04-19T12:05:34.309049Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"def SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n    \"\"\"Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n    \"\"\"\n    return Sequential(\n        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n               groups=in_channels, stride=stride, padding=padding),\n        ReLU(),\n        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n    )\n\n\ndef create_mobilenetv1_ssd(num_classes, is_test=False):\n    base_net = MobileNetV1(1001).model  # disable dropout layer\n\n    source_layer_indexes = [\n        12,\n        14,\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        )\n    ])\n\n    regression_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    classification_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, center_variance, size_variance, is_test=is_test)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.313976Z","iopub.execute_input":"2023-04-19T12:05:34.314346Z","iopub.status.idle":"2023-04-19T12:05:34.329148Z","shell.execute_reply.started":"2023-04-19T12:05:34.314289Z","shell.execute_reply":"2023-04-19T12:05:34.327919Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"create_net = create_mobilenetv1_ssd\nnet = create_net(21)\nmin_loss = -10000.0\nlast_epoch = -1","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.330948Z","iopub.execute_input":"2023-04-19T12:05:34.331703Z","iopub.status.idle":"2023-04-19T12:05:34.438907Z","shell.execute_reply.started":"2023-04-19T12:05:34.331667Z","shell.execute_reply":"2023-04-19T12:05:34.437886Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"# **MultiboxLoss**","metadata":{}},{"cell_type":"code","source":"class MultiboxLoss(nn.Module):\n    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n        \"\"\"Implement SSD Multibox Loss.\n\n        Basically, Multibox loss combines classification loss\n         and Smooth L1 regression loss.\n        \"\"\"\n        super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)\n\n    def forward(self, confidence, predicted_locations, labels, gt_locations):\n        \"\"\"Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            locations (batch_size, num_priors, 4): predicted locations.\n            labels (batch_size, num_priors): real labels of all the priors.\n            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        \"\"\"\n        num_classes = confidence.size(2)\n        with torch.no_grad():\n            # derived from cross_entropy=sum(log(p))\n            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]\n            mask = hard_negative_mining(loss, labels, self.neg_pos_ratio)\n\n        confidence = confidence[mask, :]\n        classification_loss = F.cross_entropy(confidence.reshape(-1, num_classes), labels[mask],  size_average=False)\n        pos_mask = labels > 0\n        predicted_locations = predicted_locations[pos_mask, :].reshape(-1, 4)\n        gt_locations = gt_locations[pos_mask, :].reshape(-1, 4)\n        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations,  size_average=False)\n        num_pos = gt_locations.size(0)\n        return smooth_l1_loss/num_pos, classification_loss/num_pos\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-19T12:05:34.440313Z","iopub.execute_input":"2023-04-19T12:05:34.442287Z","iopub.status.idle":"2023-04-19T12:05:34.453145Z","shell.execute_reply.started":"2023-04-19T12:05:34.442246Z","shell.execute_reply":"2023-04-19T12:05:34.452159Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n    net.train(True)\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    for i, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        confidence, locations = net(images)\n        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)  # TODO CHANGE BOXES\n        loss = regression_loss + classification_loss\n        loss.requires_grad_(True)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n        if i and i % debug_steps == 0:\n            avg_loss = running_loss / debug_steps\n            avg_reg_loss = running_regression_loss / debug_steps\n            avg_clf_loss = running_classification_loss / debug_steps\n            print(\n                f\"Epoch: {epoch}, Step: {i}, \" +\n                f\"Average Loss: {avg_loss:.4f}, \" +\n                f\"Average Regression Loss {avg_reg_loss:.4f}, \" +\n                f\"Average Classification Loss: {avg_clf_loss:.4f}\"\n            )\n            running_loss = 0.0\n            running_regression_loss = 0.0\n            running_classification_loss = 0.0\n\n\ndef test(loader, net, criterion, device):\n    net.eval()\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    num = 0\n    for _, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n        num += 1\n\n        with torch.no_grad():\n            confidence, locations = net(images)\n            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n            loss = regression_loss + classification_loss\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.454795Z","iopub.execute_input":"2023-04-19T12:05:34.455452Z","iopub.status.idle":"2023-04-19T12:05:34.469128Z","shell.execute_reply.started":"2023-04-19T12:05:34.455405Z","shell.execute_reply":"2023-04-19T12:05:34.468126Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr\nextra_layers_lr = args.extra_layers_lr if args.extra_layers_lr is not None else args.lr\nif args.freeze_base_net:\n    logging.info(\"Freeze base net.\")\n    freeze_net_layers(net.base_net)\n    params = itertools.chain(net.source_layer_add_ons.parameters(), net.extras.parameters(),\n                             net.regression_headers.parameters(), net.classification_headers.parameters())\n    params = [\n        {'params': itertools.chain(\n            net.source_layer_add_ons.parameters(),\n            net.extras.parameters()\n        ), 'lr': extra_layers_lr},\n        {'params': itertools.chain(\n            net.regression_headers.parameters(),\n            net.classification_headers.parameters()\n        )}\n    ]\nelif args.freeze_net:\n    freeze_net_layers(net.base_net)\n    freeze_net_layers(net.source_layer_add_ons)\n    freeze_net_layers(net.extras)\n    params = itertools.chain(net.regression_headers.parameters(), net.classification_headers.parameters())\n    logging.info(\"Freeze all the layers except prediction heads.\")\nelse:\n    params = [\n        {'params': net.base_net.parameters(), 'lr': base_net_lr},\n        {'params': itertools.chain(\n            net.source_layer_add_ons.parameters(),\n            net.extras.parameters()\n        ), 'lr': extra_layers_lr},\n        {'params': itertools.chain(\n            net.regression_headers.parameters(),\n            net.classification_headers.parameters()\n        )}\n    ]","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.470620Z","iopub.execute_input":"2023-04-19T12:05:34.471159Z","iopub.status.idle":"2023-04-19T12:05:34.483560Z","shell.execute_reply.started":"2023-04-19T12:05:34.471121Z","shell.execute_reply":"2023-04-19T12:05:34.482358Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"timer = Timer()\ntimer.start(\"Load Model\")\nif args.resume:\n    print(f\"Resume from the model {args.resume}\")\n    net.load(args.resume)\nelif args.base_net:\n    print(f\"Init from base net {args.base_net}\")\n    net.init_from_base_net(args.base_net)\nelif args.pretrained_ssd:\n    print(f\"Init from pretrained ssd {args.pretrained_ssd}\")\n    net.init_from_pretrained_ssd(args.pretrained_ssd)\nprint(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n\nnet.to(DEVICE)\n\noptimizer = torch.optim.SGD(params, lr=args.lr, momentum=args.momentum,\n                            weight_decay=args.weight_decay)\ncriterion = MultiboxLoss(priors, iou_threshold=0.5, neg_pos_ratio=3,\n                         center_variance=0.1, size_variance=0.2, device=DEVICE)\n\nprint(f\"Learning rate: {args.lr}, Base net learning rate: {base_net_lr}, \"\n             + f\"Extra Layers learning rate: {extra_layers_lr}.\")\n\nif args.scheduler == 'multi-step':\n    print(\"Uses MultiStepLR scheduler.\")\n    milestones = [int(v.strip()) for v in args.milestones.split(\",\")]\n    scheduler = MultiStepLR(optimizer, milestones=milestones,\n                                                 gamma=0.1, last_epoch=last_epoch)\nelif args.scheduler == 'cosine':\n    print(\"Uses CosineAnnealingLR scheduler.\")\n    scheduler = CosineAnnealingLR(optimizer, args.t_max, last_epoch=last_epoch)\nelse:\n    logging.fatal(f\"Unsupported Scheduler: {args.scheduler}.\")\n    parser.print_help(sys.stderr)\n    sys.exit(1)\n\nprint(f\"Start training from epoch {last_epoch + 1}.\")\nfor epoch in range(last_epoch + 1, args.num_epochs):\n    scheduler.step()\n    train(train_loader, net, criterion, optimizer,\n          device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)\n\n    if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:\n        val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n        print(\n            f\"Epoch: {epoch}, \" +\n            f\"Validation Loss: {val_loss:.4f}, \" +\n            f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n            f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n        )\n        model_path = os.path.join('/kaggle/working/', f\"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n        net.save(model_path)\n        #print(f\"Saved model {model_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T12:05:34.484921Z","iopub.execute_input":"2023-04-19T12:05:34.486062Z","iopub.status.idle":"2023-04-19T14:41:37.140775Z","shell.execute_reply.started":"2023-04-19T12:05:34.485987Z","shell.execute_reply":"2023-04-19T14:41:37.138619Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"Took 0.00 seconds to load the model.\nLearning rate: 0.001, Base net learning rate: 0.001, Extra Layers learning rate: 0.001.\nUses MultiStepLR scheduler.\nStart training from epoch 0.\nEpoch: 0, Step: 100, Average Loss: 24.4116, Average Regression Loss 12.4447, Average Classification Loss: 11.9670\nEpoch: 0, Validation Loss: 12.3403, Validation Regression Loss 2.8261, Validation Classification Loss: 9.5142\nEpoch: 1, Step: 100, Average Loss: 18.2912, Average Regression Loss 10.8685, Average Classification Loss: 7.4227\nEpoch: 2, Step: 100, Average Loss: 15.6993, Average Regression Loss 10.6252, Average Classification Loss: 5.0741\nEpoch: 3, Step: 100, Average Loss: 15.6479, Average Regression Loss 10.6674, Average Classification Loss: 4.9806\nEpoch: 4, Step: 100, Average Loss: 15.2757, Average Regression Loss 10.3754, Average Classification Loss: 4.9004\nEpoch: 5, Step: 100, Average Loss: 15.3936, Average Regression Loss 10.5212, Average Classification Loss: 4.8724\nEpoch: 6, Step: 100, Average Loss: 14.8303, Average Regression Loss 9.9655, Average Classification Loss: 4.8648\nEpoch: 7, Step: 100, Average Loss: 14.9772, Average Regression Loss 10.1655, Average Classification Loss: 4.8117\nEpoch: 8, Step: 100, Average Loss: 14.7018, Average Regression Loss 9.8902, Average Classification Loss: 4.8116\nEpoch: 9, Step: 100, Average Loss: 14.7410, Average Regression Loss 9.9391, Average Classification Loss: 4.8019\nEpoch: 10, Step: 100, Average Loss: 15.1617, Average Regression Loss 10.3796, Average Classification Loss: 4.7821\nEpoch: 11, Step: 100, Average Loss: 14.8419, Average Regression Loss 10.0879, Average Classification Loss: 4.7540\nEpoch: 12, Step: 100, Average Loss: 14.8726, Average Regression Loss 10.1162, Average Classification Loss: 4.7563\nEpoch: 13, Step: 100, Average Loss: 14.8344, Average Regression Loss 10.0787, Average Classification Loss: 4.7557\nEpoch: 14, Step: 100, Average Loss: 14.7281, Average Regression Loss 9.9871, Average Classification Loss: 4.7410\nEpoch: 15, Step: 100, Average Loss: 14.7373, Average Regression Loss 10.0110, Average Classification Loss: 4.7263\nEpoch: 16, Step: 100, Average Loss: 14.7892, Average Regression Loss 10.0735, Average Classification Loss: 4.7157\nEpoch: 17, Step: 100, Average Loss: 14.7404, Average Regression Loss 10.0195, Average Classification Loss: 4.7209\nEpoch: 18, Step: 100, Average Loss: 14.4320, Average Regression Loss 9.7187, Average Classification Loss: 4.7133\nEpoch: 19, Step: 100, Average Loss: 14.6774, Average Regression Loss 9.9640, Average Classification Loss: 4.7134\nEpoch: 20, Step: 100, Average Loss: 14.3280, Average Regression Loss 9.6445, Average Classification Loss: 4.6834\nEpoch: 21, Step: 100, Average Loss: 14.5897, Average Regression Loss 9.8949, Average Classification Loss: 4.6947\nEpoch: 22, Step: 100, Average Loss: 14.5166, Average Regression Loss 9.8534, Average Classification Loss: 4.6632\nEpoch: 23, Step: 100, Average Loss: 14.7503, Average Regression Loss 10.0658, Average Classification Loss: 4.6845\nEpoch: 24, Step: 100, Average Loss: 14.8422, Average Regression Loss 10.1632, Average Classification Loss: 4.6791\nEpoch: 25, Step: 100, Average Loss: 14.6994, Average Regression Loss 10.0259, Average Classification Loss: 4.6735\nEpoch: 26, Step: 100, Average Loss: 14.5744, Average Regression Loss 9.8927, Average Classification Loss: 4.6817\nEpoch: 27, Step: 100, Average Loss: 14.1548, Average Regression Loss 9.4962, Average Classification Loss: 4.6586\nEpoch: 28, Step: 100, Average Loss: 14.4382, Average Regression Loss 9.7722, Average Classification Loss: 4.6660\nEpoch: 29, Step: 100, Average Loss: 14.5412, Average Regression Loss 9.8839, Average Classification Loss: 4.6573\nEpoch: 30, Step: 100, Average Loss: 14.6109, Average Regression Loss 9.9587, Average Classification Loss: 4.6522\nEpoch: 31, Step: 100, Average Loss: 14.6386, Average Regression Loss 9.9955, Average Classification Loss: 4.6431\nEpoch: 32, Step: 100, Average Loss: 14.6089, Average Regression Loss 9.9772, Average Classification Loss: 4.6317\nEpoch: 33, Step: 100, Average Loss: 14.3222, Average Regression Loss 9.6962, Average Classification Loss: 4.6260\nEpoch: 34, Step: 100, Average Loss: 14.7351, Average Regression Loss 10.1131, Average Classification Loss: 4.6220\nEpoch: 35, Step: 100, Average Loss: 14.5907, Average Regression Loss 9.9680, Average Classification Loss: 4.6227\nEpoch: 36, Step: 100, Average Loss: 14.5892, Average Regression Loss 9.9572, Average Classification Loss: 4.6321\nEpoch: 37, Step: 100, Average Loss: 14.6150, Average Regression Loss 10.0041, Average Classification Loss: 4.6109\nEpoch: 38, Step: 100, Average Loss: 14.5638, Average Regression Loss 9.9146, Average Classification Loss: 4.6492\nEpoch: 39, Step: 100, Average Loss: 14.4994, Average Regression Loss 9.8780, Average Classification Loss: 4.6214\nEpoch: 40, Step: 100, Average Loss: 14.4809, Average Regression Loss 9.8595, Average Classification Loss: 4.6214\nEpoch: 41, Step: 100, Average Loss: 14.4845, Average Regression Loss 9.8845, Average Classification Loss: 4.6000\nEpoch: 42, Step: 100, Average Loss: 14.4083, Average Regression Loss 9.8025, Average Classification Loss: 4.6059\nEpoch: 43, Step: 100, Average Loss: 14.2989, Average Regression Loss 9.7045, Average Classification Loss: 4.5944\nEpoch: 44, Step: 100, Average Loss: 14.4778, Average Regression Loss 9.8963, Average Classification Loss: 4.5814\nEpoch: 45, Step: 100, Average Loss: 14.3389, Average Regression Loss 9.7576, Average Classification Loss: 4.5812\nEpoch: 46, Step: 100, Average Loss: 14.5273, Average Regression Loss 9.9390, Average Classification Loss: 4.5883\nEpoch: 47, Step: 100, Average Loss: 14.5409, Average Regression Loss 9.9672, Average Classification Loss: 4.5737\nEpoch: 48, Step: 100, Average Loss: 14.5549, Average Regression Loss 9.9584, Average Classification Loss: 4.5965\nEpoch: 49, Step: 100, Average Loss: 14.5803, Average Regression Loss 9.9966, Average Classification Loss: 4.5837\nEpoch: 49, Validation Loss: 7.3531, Validation Regression Loss 2.6393, Validation Classification Loss: 4.7138\n","output_type":"stream"}]}]}